{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificaci√≥n MNIST con red convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST es un ejemplo cl√°sico de reconocimiento de d√≠gitos escritos a mano. Se utiliza la base de datos MNIST que contiene 60,000 im√°genes de entrenamiento y 10,000 im√°genes de prueba. Cada imagen es de 28x28 pixeles y cada pixel tiene un valor entre 0 y 255.\n",
    "\n",
    "Este dataset marc√≥ un hito en la historia de la IA, con el que [en 1998 el equipo de Yann LeCun utiliz√≥ una red neuronal convolucional para conseguir un error de 0.8% en el reconocimiento de d√≠gitos](https://www.youtube.com/watch?v=H0oEr40YhrQ), usando la arquitectura LeNet-5.\n",
    "\n",
    "Es el mismo ejemplo con el que se explica la [teor√≠a sobre redes neuronales en el video de 3Brown1Blue](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) ([versi√≥n doblada a espa√±ol](https://www.youtube.com/watch?v=jKCQsndqEGQ))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A menudo usaremos m√°s de una transformaci√≥n para preprocesar los datos. Por ejemplo, en el caso de las im√°genes, a menudo se normalizan y se redimensionan. Para hacer esto de manera eficiente, podemos usar la clase `Compose` de `torchvision.transforms`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de clases en MNIST: 10\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Transformaciones para convertir a tensor\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Cargamos el dataset MNIST\n",
    "train_data_full = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# N√∫mero total de muestras de entrenamiento\n",
    "num_train = len(train_data_full)\n",
    "val_ratio = 0.2  # Por ejemplo, 20% para validaci√≥n\n",
    "\n",
    "# Tama√±os de split\n",
    "num_val = int(num_train * val_ratio)\n",
    "num_train = num_train - num_val\n",
    "\n",
    "# Divisi√≥n de datos en entrenamiento y validaci√≥n\n",
    "train_data, val_data = random_split(train_data_full, [num_train, num_val])\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# N√∫mero de clases\n",
    "num_classes = len(train_data_full.classes)  # o tambi√©n: len(train_data_full.classes)\n",
    "print(f\"N√∫mero de clases en MNIST: {num_classes}\")\n",
    "\n",
    "#quiero dividir x e y de val_data\n",
    "def split_data(data):\n",
    "    x = torch.stack([item[0] for item in data])\n",
    "    y = torch.tensor([item[1] for item in data])\n",
    "    return x, y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definici√≥n del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, conv_filters, hidden_layers, hidden_units, output_size=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, conv_filters, kernel_size=3, padding=1)   # (28x28) -> (28x28)\n",
    "        self.conv2 = nn.Conv2d(conv_filters, conv_filters * 2, kernel_size=3, padding=1)  # (14x14) -> (14x14)\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # reduce a la mitad cada vez\n",
    "\n",
    "        self.flattened_size = (conv_filters * 2) * 7 * 7  # despu√©s de 2 poolings\n",
    "\n",
    "        fc_layers = []\n",
    "        in_features = self.flattened_size\n",
    "        for _ in range(hidden_layers):\n",
    "            fc_layers.append(nn.Linear(in_features, hidden_units))\n",
    "            fc_layers.append(nn.ReLU())\n",
    "            in_features = hidden_units\n",
    "        fc_layers.append(nn.Linear(in_features, output_size))\n",
    "        self.fc_net = nn.Sequential(*fc_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 28x28 ‚Üí 14x14\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 14x14 ‚Üí 7x7\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_net(x)\n",
    "        return x\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definici√≥n de la funci√≥n de p√©rdida y el optimizador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la funci√≥n de perdida y el optimizador. En este caso usaremos el optimizador `optim.Adam`. Adam es una variante del descenso de gradiente estoc√°stico que calcula tasas de aprendizaje individuales para diferentes par√°metros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid de hiperpar√°metros\n",
    "param_grid = {\n",
    "    'conv_filters': [16, 32],\n",
    "    'hidden_layers': [1, 2],\n",
    "    'hidden_units': [64, 128],\n",
    "    'lr': [0.001],\n",
    "    'batch_size': [64]\n",
    "}\n",
    "grid = list(ParameterGrid(param_grid))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Probando configuraci√≥n: {'batch_size': 64, 'conv_filters': 16, 'hidden_layers': 1, 'hidden_units': 64, 'lr': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 750/750 [00:30<00:00, 24.79it/s, loss=0.125] \n",
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 750/750 [00:31<00:00, 23.66it/s, loss=0.055]  \n",
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 750/750 [00:34<00:00, 21.56it/s, loss=0.0443] \n",
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 750/750 [00:33<00:00, 22.39it/s, loss=0.0352] \n",
      "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 750/750 [00:31<00:00, 23.95it/s, loss=0.00384] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Precisi√≥n en validaci√≥n: 0.9840\n",
      "\n",
      "üîç Probando configuraci√≥n: {'batch_size': 64, 'conv_filters': 16, 'hidden_layers': 1, 'hidden_units': 128, 'lr': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 750/750 [00:31<00:00, 24.04it/s, loss=0.0261]\n",
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 750/750 [00:32<00:00, 23.25it/s, loss=0.0187] \n",
      "Epoch 3:   4%|‚ñç         | 33/750 [00:01<00:30, 23.30it/s, loss=0.0654] "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "best_acc = 0.0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"\\nüîç Probando configuraci√≥n: {params}\")\n",
    "    model = CNN(conv_filters=params['conv_filters'],\n",
    "                hidden_layers=params['hidden_layers'],\n",
    "                hidden_units=params['hidden_units']).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "    # Entrenamiento simple\n",
    "    for epoch in range(5):  # ajustable\n",
    "        model.train()\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=True)\n",
    "        for batch_x, batch_y in loop:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Validaci√≥n\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            preds = outputs.argmax(dim=1) # obtener la clase con mayor probabilidad\n",
    "            all_preds.extend(preds.cpu().numpy()) # convertir a numpy y extender la lista\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"‚úÖ Precisi√≥n en validaci√≥n: {acc:.4f}\")\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_params = params\n",
    "        best_model = model\n",
    "\n",
    "# ===========================\n",
    "# RESULTADO FINAL\n",
    "# ===========================\n",
    "print(\"\\nüèÜ Mejor configuraci√≥n encontrada:\")\n",
    "print(best_params)\n",
    "print(f\"Precisi√≥n de validaci√≥n: {best_acc:.4f}\")\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "torch.save(best_model.state_dict(), 'best_cnn_mnist.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluaci√≥n del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 99.01%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  for images, labels in test_loader:\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "  print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://dudeperf3ct.github.io/cnn/mnist/2018/10/17/Force-of-Convolutional-Neural-Networks/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
