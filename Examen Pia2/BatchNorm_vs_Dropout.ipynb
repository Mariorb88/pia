{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d05eeb46",
   "metadata": {},
   "source": [
    "\n",
    "# ğŸ“˜ Batch Normalization y Dropout en PyTorch\n",
    "\n",
    "Este notebook explica de forma didÃ¡ctica quÃ© son y cÃ³mo se usan **Batch Normalization** y **Dropout**, dos tÃ©cnicas fundamentales para mejorar el entrenamiento y la generalizaciÃ³n de redes neuronales.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d9778e",
   "metadata": {},
   "source": [
    "\n",
    "## ğŸ”¹ Batch Normalization\n",
    "\n",
    "**Batch Normalization** es una tÃ©cnica que normaliza las activaciones dentro de la red, usando estadÃ­sticas del batch (media y desviaciÃ³n estÃ¡ndar).\n",
    "\n",
    "### Â¿QuÃ© consigue?\n",
    "- Acelera el entrenamiento\n",
    "- Permite usar learning rates mÃ¡s altos\n",
    "- Reduce el problema del *internal covariate shift*\n",
    "- Ayuda (ligeramente) a la regularizaciÃ³n\n",
    "\n",
    "### Ejemplo en PyTorch:\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "# Para capas lineales\n",
    "bn1d = nn.BatchNorm1d(num_features=128)\n",
    "\n",
    "# Para capas convolucionales\n",
    "bn2d = nn.BatchNorm2d(num_features=64)\n",
    "```\n",
    "\n",
    "Se suele usar **despuÃ©s de la capa** y **antes de la funciÃ³n de activaciÃ³n**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a222896",
   "metadata": {},
   "source": [
    "\n",
    "## ğŸ”¸ Dropout\n",
    "\n",
    "**Dropout** es una tÃ©cnica de regularizaciÃ³n que apaga aleatoriamente neuronas durante el entrenamiento.\n",
    "\n",
    "### Â¿QuÃ© consigue?\n",
    "- Reduce el sobreajuste\n",
    "- Fuerza al modelo a no depender de neuronas concretas\n",
    "- Se comporta como un *ensemble* implÃ­cito\n",
    "\n",
    "### Ejemplo en PyTorch:\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "# Con p = 0.5 (50% de neuronas apagadas)\n",
    "dropout = nn.Dropout(p=0.5)\n",
    "```\n",
    "\n",
    "Se aplica tÃ­picamente **despuÃ©s de activaciones** y **solo en entrenamiento**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18234fd",
   "metadata": {},
   "source": [
    "\n",
    "## âš™ï¸ Â¿Se pueden usar juntos?\n",
    "\n",
    "Â¡SÃ­! Y de hecho es comÃºn combinarlos:\n",
    "\n",
    "```python\n",
    "x = F.relu(self.bn1(self.fc1(x)))\n",
    "x = self.dropout(x)\n",
    "```\n",
    "\n",
    "1. Se normaliza la salida de `fc1`\n",
    "2. Se aplica la funciÃ³n de activaciÃ³n ReLU\n",
    "3. Se aplica Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe41f81",
   "metadata": {},
   "source": [
    "\n",
    "## ğŸ“Š ComparaciÃ³n rÃ¡pida\n",
    "\n",
    "| CaracterÃ­stica            | BatchNorm                     | Dropout                        |\n",
    "|---------------------------|-------------------------------|--------------------------------|\n",
    "| Â¿CuÃ¡ndo se aplica?        | Entre capas y activaciones     | DespuÃ©s de activaciones        |\n",
    "| Â¿Mejora estabilidad?      | âœ… SÃ­                          | âŒ No, es regularizador         |\n",
    "| Â¿Reduce sobreajuste?      | Un poco                       | âœ… SÃ­, muy Ãºtil                 |\n",
    "| Â¿Se activa en inferencia? | âŒ No                          | âŒ No                           |\n",
    "| Â¿Afecta al forward pass?  | SÃ­, ajusta valores             | SÃ­, elimina neuronas aleatoriamente |\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}