{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d05eeb46",
   "metadata": {},
   "source": [
    "\n",
    "# 📘 Batch Normalization y Dropout en PyTorch\n",
    "\n",
    "Este notebook explica de forma didáctica qué son y cómo se usan **Batch Normalization** y **Dropout**, dos técnicas fundamentales para mejorar el entrenamiento y la generalización de redes neuronales.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d9778e",
   "metadata": {},
   "source": [
    "\n",
    "## 🔹 Batch Normalization\n",
    "\n",
    "**Batch Normalization** es una técnica que normaliza las activaciones dentro de la red, usando estadísticas del batch (media y desviación estándar).\n",
    "\n",
    "### ¿Qué consigue?\n",
    "- Acelera el entrenamiento\n",
    "- Permite usar learning rates más altos\n",
    "- Reduce el problema del *internal covariate shift*\n",
    "- Ayuda (ligeramente) a la regularización\n",
    "\n",
    "### Ejemplo en PyTorch:\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "# Para capas lineales\n",
    "bn1d = nn.BatchNorm1d(num_features=128)\n",
    "\n",
    "# Para capas convolucionales\n",
    "bn2d = nn.BatchNorm2d(num_features=64)\n",
    "```\n",
    "\n",
    "Se suele usar **después de la capa** y **antes de la función de activación**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a222896",
   "metadata": {},
   "source": [
    "\n",
    "## 🔸 Dropout\n",
    "\n",
    "**Dropout** es una técnica de regularización que apaga aleatoriamente neuronas durante el entrenamiento.\n",
    "\n",
    "### ¿Qué consigue?\n",
    "- Reduce el sobreajuste\n",
    "- Fuerza al modelo a no depender de neuronas concretas\n",
    "- Se comporta como un *ensemble* implícito\n",
    "\n",
    "### Ejemplo en PyTorch:\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "# Con p = 0.5 (50% de neuronas apagadas)\n",
    "dropout = nn.Dropout(p=0.5)\n",
    "```\n",
    "\n",
    "Se aplica típicamente **después de activaciones** y **solo en entrenamiento**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18234fd",
   "metadata": {},
   "source": [
    "\n",
    "## ⚙️ ¿Se pueden usar juntos?\n",
    "\n",
    "¡Sí! Y de hecho es común combinarlos:\n",
    "\n",
    "```python\n",
    "x = F.relu(self.bn1(self.fc1(x)))\n",
    "x = self.dropout(x)\n",
    "```\n",
    "\n",
    "1. Se normaliza la salida de `fc1`\n",
    "2. Se aplica la función de activación ReLU\n",
    "3. Se aplica Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe41f81",
   "metadata": {},
   "source": [
    "\n",
    "## 📊 Comparación rápida\n",
    "\n",
    "| Característica            | BatchNorm                     | Dropout                        |\n",
    "|---------------------------|-------------------------------|--------------------------------|\n",
    "| ¿Cuándo se aplica?        | Entre capas y activaciones     | Después de activaciones        |\n",
    "| ¿Mejora estabilidad?      | ✅ Sí                          | ❌ No, es regularizador         |\n",
    "| ¿Reduce sobreajuste?      | Un poco                       | ✅ Sí, muy útil                 |\n",
    "| ¿Se activa en inferencia? | ❌ No                          | ❌ No                           |\n",
    "| ¿Afecta al forward pass?  | Sí, ajusta valores             | Sí, elimina neuronas aleatoriamente |\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}