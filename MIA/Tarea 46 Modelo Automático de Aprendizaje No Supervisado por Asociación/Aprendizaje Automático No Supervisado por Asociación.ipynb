{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPKfT94HSxlh"
      },
      "source": [
        "# **Aprendizaje Automático No Supervisado por Asociación**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ57d-pYhb00",
        "outputId": "89cd6edc-f7e0-490f-dcfc-d15e35642c41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database\n",
            "License(s): CC0-1.0\n"
          ]
        }
      ],
      "source": [
        "#--------------------------------------------------------------------------------\n",
        "# Paso 0. Kaggle API (opcional si decides usar la API)\n",
        "#--------------------------------------------------------------------------------\n",
        "# Instalamos las librerías mlxtend y kaggle\n",
        "#--------------------------------------------------------------------------------\n",
        "!pip install mlxtend --quiet\n",
        "!pip install kaggle --quiet\n",
        "\n",
        "#---------------------------------------------------------------------------------\n",
        "# Cargamos las librerías necesarias\n",
        "#---------------------------------------------------------------------------------\n",
        "import zipfile\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Descargamos el dataset desde Kaggle (opcional si ya tienes el dataset descargado)\n",
        "#--------------------------------------------------------------------------------\n",
        "!kaggle datasets download -d uciml/pima-indians-diabetes-database --quiet\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Creamos la carpeta que va a contener el fichero descargado\n",
        "#--------------------------------------------------------------------------------\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Extraemos los datos\n",
        "#--------------------------------------------------------------------------------\n",
        "with zipfile.ZipFile( \"pima-indians-diabetes-database.zip\", \"r\" ) as zip_ref:\n",
        "    zip_ref.extractall( \"dataset\" )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "i6o4J7YrXq2q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Reglas de asociación filtradas y ordenadas:\n",
            "\n",
            "Antecedentes                          Consecuencias   Soporte   Confianza   Fuerza de asociación\n",
            "----------------------------------------------------------------------------------------------\n",
            "( Glucose                          ) → (BMI        )    25.50%     81.99%           1.25\n",
            "( Outcome                          ) → (BMI        )    40.40%     80.80%           1.23\n",
            "( Glucose, Outcome                 ) → (BMI        )    21.80%     87.55%           1.33\n",
            "( DiabetesPedigreeFunction, Outcome) → (BMI        )    21.00%     86.78%           1.32\n",
            "( Glucose                          ) → (Outcome    )    24.90%     80.06%           1.60\n",
            "( Glucose, BMI                     ) → (Outcome    )    21.80%     85.49%           1.71\n"
          ]
        }
      ],
      "source": [
        "#---------------------------------------------------------------------------------\n",
        "# Paso 1: Importamos de Bibliotecas\n",
        "#---------------------------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "from collections                import Counter\n",
        "from imblearn.over_sampling     import SMOTE\n",
        "from mlxtend.frequent_patterns  import apriori, association_rules\n",
        "\n",
        "# Suprimimos las advertencias específicas de deprecación\n",
        "warnings.filterwarnings( \"ignore\", category = DeprecationWarning )\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 2. Cargamos los datos en un DataFrame\n",
        "#--------------------------------------------------------------------------------\n",
        "df = pd.read_csv( \"dataset/diabetes.csv\" )\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 3. Seleccionamos variables relevantes para Apriori\n",
        "#--------------------------------------------------------------------------------\n",
        "df_bin = df[ [ 'Glucose', 'BMI', 'Age', 'DiabetesPedigreeFunction', 'Outcome' ] ].copy()\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 4. Convertimos las variables a valores binarios (mantener como booleanos)\n",
        "#--------------------------------------------------------------------------------\n",
        "df_bin[ 'Glucose' ]                  = df_bin[ 'Glucose'                  ].apply(lambda x: 1 if x >= 140 else 0)\n",
        "df_bin[ 'BMI' ]                      = df_bin[ 'BMI'                      ].apply(lambda x: 1 if x >=  30 else 0)\n",
        "df_bin[ 'Age' ]                      = df_bin[ 'Age'                      ].apply(lambda x: 1 if x >=  50 else 0)\n",
        "df_bin[ 'DiabetesPedigreeFunction' ] = df_bin[ 'DiabetesPedigreeFunction' ].apply(lambda x: 1 if x >= 0.5 else 0)\n",
        "df_bin[ 'Outcome' ]                  = df_bin[ 'Outcome'                  ].apply(lambda x: 1 if x ==   1 else 0)\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 5. Nivelamos las clases usando SMOTE\n",
        "#   La técnica SMOTE ayuda a balancear el conjunto de datos para evitar que el modelo favorezca la clase mayoritaria\n",
        "#--------------------------------------------------------------------------------\n",
        "X = df_bin.drop( 'Outcome', axis = 1 )  # Eliminar la columna 'Outcome' de las características\n",
        "y = df_bin[ 'Outcome' ]                 # La columna 'Outcome' es el objetivo\n",
        "\n",
        "# Aplicamos SMOTE para sobremuestrear las clases\n",
        "smote = SMOTE( random_state = 42 )\n",
        "X_res, y_res = smote.fit_resample( X, y )\n",
        "\n",
        "# Creamos el DataFrame balanceado\n",
        "balanced_df = pd.DataFrame( X_res, columns = X.columns )\n",
        "balanced_df[ 'Outcome' ] = y_res\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 6. Generamos los conjuntos de elementos más frecuentes con Apriori\n",
        "#--------------------------------------------------------------------------------\n",
        "#   df            DataFrame con datos en formato binario\n",
        "#   min_support   Umbral mínimo de soporte para considerar conjuntos frecuentes\n",
        "#                   valores altos (0.3  a 0.5) generan pocas reglas pero muy relevantes\n",
        "#                   valores bajos (0.05 a 0.1) generan muchas reglas pero algunas poco significativas o con ruido\n",
        "#   use_colnames  Usa los nombres de las columnas en lugar de índices numéricos\n",
        "#   max_len       Tamaño máximo del conjunto de items generados.\n",
        "#   verbose       Muestra el progreso del algoritmo\n",
        "#   low_memory    Usa menos memoria (útil ante grandes conjuntos de datos)\n",
        "#   n_jobs        Número de procesadores en paralelo (-1 usa todos los disponibles) NO EXISTE EN mlxtend\n",
        "#--------------------------------------------------------------------------------\n",
        "conjuntoElementosFrecuentes = apriori(\n",
        "    balanced_df.round().astype( int ), # aseguramos valores binarios\n",
        "    min_support   = 0.2,\n",
        "    max_len       = 5,\n",
        "    verbose       = False,\n",
        "    use_colnames  = True\n",
        "    )\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 7. Generamos las reglas de asociación con association_rules en función de los conjuntos de elementos más frecuentes\n",
        "#--------------------------------------------------------------------------------\n",
        "#   frequent_itemsets   Contiene los conjuntos de elementos frecuentes y su soporte\n",
        "#   metric              Métricas que ayudan a evaluar la calidad de las reglas de asociación\n",
        "#                           confidence    probabilidad de que al ser cierto el Antecedente también lo sea el Consequente\n",
        "#                             cerca de 1    Fuerte relación entre Antecedente y Consecuente\n",
        "#                             valor bajo    Antecedente ocurre muchas veces sin que ocurra Consecuente\n",
        "#                           lift          probabilidad de que el Consecuente ocurra si el Antecedente es cierto, en comparación con la probabilidad general del Consecuente\n",
        "#                             > 1   Antecedente y Consecuente positivamente correlacionados\n",
        "#                             = 1   Antecedente y Consecuente son independientes\n",
        "#                             < 1   Antecedente reduce la probabilidad de Consecuente\n",
        "#                           support       mide la frecuencia con la que ocurre un conjunto de valores en el dataset\n",
        "#                             > 0.5   Regla más común y confiable\n",
        "#                             < 0.5   Puede ser un patrón raro pero valioso\n",
        "#                           leverage      mide la frecuencia de que Antecedente y Consecuente ocurran juntos en comparación de si fueran independientes\n",
        "#                             > 0   Antecedente y Consecuente aparecen juntos más de lo esperado\n",
        "#                             = 0   NO hay relación\n",
        "#                             < 0   Antecedente y Consecuente aparecen juntos menos de lo esperado\n",
        "#                           conviction    mide la dependencia del Consecuente frente al Antecedente comparado con que ocurra el Consecuente por sí solo\n",
        "#                             > 1           Relación fuerte en Antecedente y Consecuente\n",
        "#                             cercanos a 1  Relación débil\n",
        "#   min_threshold       Mantiene reglas con el nivel de la métrica especificado\n",
        "#--------------------------------------------------------------------------------\n",
        "#Reglas = association_rules(\n",
        "#    conjuntoElementosFrecuentes,\n",
        "#    metric        = \"confidence\",\n",
        "#    min_threshold = 0.2 )\n",
        "\n",
        "# Generamos ahora todas las reglas\n",
        "Reglas = association_rules( conjuntoElementosFrecuentes )\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 8. Filtramos por las reglas más importantes:\n",
        "#     confidence > 0.2\n",
        "#     lift       > 1.2\n",
        "#--------------------------------------------------------------------------------\n",
        "# Filtramos las reglas que cumplen con los valores de confidence y lift\n",
        "#reglasFiltradas = Reglas[ ( Reglas[ 'confidence' ] > 0.2 ) & ( Reglas[ 'lift' ] > 1.2 ) ]\n",
        "reglasFiltradas = Reglas[ ( Reglas[ 'confidence' ] > 0.150 ) ]\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 9. Filtramos ahora sólo por las columnas necesarias\n",
        "#--------------------------------------------------------------------------------\n",
        "reglasFiltradas = reglasFiltradas[ [ 'antecedents', 'consequents', 'support', 'confidence', 'lift' ] ].copy()\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 10. Convertimos antecedentes y consecuentes al formato deseado\n",
        "#--------------------------------------------------------------------------------\n",
        "reglasFiltradas[ 'antecedents' ] = reglasFiltradas[ 'antecedents' ].apply( lambda x: ', '.join( [ str( item ) for item in x ] ) )\n",
        "reglasFiltradas[ 'consequents' ] = reglasFiltradas[ 'consequents' ].apply( lambda x: ', '.join( [ str( item ) for item in x ] ) )\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 11. Calculamos el tamaño máximo de los valores en 'antecedents' y 'consequents'\n",
        "#--------------------------------------------------------------------------------\n",
        "maxAntecedents, maxConsequents = 16, 11\n",
        "if not reglasFiltradas.empty:\n",
        "    maxAntecedents = max( max( reglasFiltradas[ 'antecedents' ].apply( len ) ), 16 )\n",
        "    maxConsequents = max( max( reglasFiltradas[ 'consequents' ].apply( len ) ), 11 )\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 12. Filtramos las reglas donde \"Consequents\" sólo contenga \"Outcome\"\n",
        "#--------------------------------------------------------------------------------\n",
        "# Verificamos si \"Outcome\" es el único valor en \"Consequents\"\n",
        "reglasFiltradas[ 'only_outcome' ] = reglasFiltradas[ 'consequents' ].apply(lambda x: x == \"Outcome\" )\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 13. Ordenamos las reglas para que las que contienen sólo \"Outcome\" y en \"Consequents\" aparezcan al final\n",
        "#--------------------------------------------------------------------------------\n",
        "reglasFiltradasOrdenadas = reglasFiltradas.sort_values( by = 'only_outcome', ascending = True )\n",
        "\n",
        "# Eliminar la columna \"only_outcome\" ya que no es necesaria para la visualización\n",
        "reglasFiltradasOrdenadas = reglasFiltradasOrdenadas.drop( columns = [ 'only_outcome' ] )\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 14. Mostramos los encabezados con tamaños dinámicos basados en el tamaño máximo de las listas\n",
        "#--------------------------------------------------------------------------------\n",
        "print( \"\\nReglas de asociación filtradas y ordenadas:\\n\" )\n",
        "print( f\"{'Antecedentes':<{maxAntecedents}}     {'Consecuencias':<{maxConsequents}}   {'Soporte':<10}{'Confianza':<12}{'Fuerza de asociación':<10}\")\n",
        "print( \"-\" * (maxAntecedents + maxConsequents + 50 ) )\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 15. Imprimimos las reglas ordenadas con los valores correspondientes\n",
        "#--------------------------------------------------------------------------------\n",
        "if reglasFiltradasOrdenadas.empty:\n",
        "  print( \"No se ha encontrado ninguna regla que cumpla con los criterios.\" )\n",
        "else:\n",
        "  for index, row in reglasFiltradasOrdenadas.iterrows():\n",
        "    print( f\"( {row[ 'antecedents' ]:<{maxAntecedents}}) → ({row['consequents']:<{maxConsequents}})    {row['support']*100:.2f}%     {row['confidence']*100:.2f}%           {row['lift']:.2f}\" )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Reglas de asociación filtradas y ordenadas:\n",
            "\n",
            "Antecedentes                          Consecuencias   Soporte   Confianza   Fuerza de asociación\n",
            "----------------------------------------------------------------------------------------------\n",
            "( Glucose                          ) → (BMI        )    25.50%     81.99%           1.25\n",
            "( Outcome                          ) → (BMI        )    40.40%     80.80%           1.23\n",
            "( Glucose, DiabetesPedigreeFunction) → (BMI        )    13.00%     90.91%           1.38\n",
            "( Glucose, Outcome                 ) → (BMI        )    21.80%     87.55%           1.33\n",
            "( DiabetesPedigreeFunction, Outcome) → (BMI        )    21.00%     86.78%           1.32\n",
            "( Glucose                          ) → (Outcome    )    24.90%     80.06%           1.60\n",
            "( Glucose, BMI                     ) → (Outcome    )    21.80%     85.49%           1.71\n",
            "( Glucose, DiabetesPedigreeFunction) → (Outcome    )    12.00%     83.92%           1.68\n"
          ]
        }
      ],
      "source": [
        "#---------------------------------------------------------------------------------\n",
        "# Paso 1: Importamos de Bibliotecas\n",
        "#---------------------------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "from collections                import Counter\n",
        "from imblearn.over_sampling     import SMOTE\n",
        "from mlxtend.frequent_patterns  import apriori, association_rules\n",
        "\n",
        "# Suprimimos las advertencias específicas de deprecación\n",
        "warnings.filterwarnings( \"ignore\", category = DeprecationWarning )\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 2. Cargamos los datos en un DataFrame\n",
        "#--------------------------------------------------------------------------------\n",
        "df = pd.read_csv( \"dataset/diabetes.csv\" )\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 3. Seleccionamos variables relevantes para Apriori\n",
        "#--------------------------------------------------------------------------------\n",
        "df_bin = df[ [ 'Glucose', 'BMI', 'Age', 'DiabetesPedigreeFunction', 'Outcome' ] ].copy()\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 4. Convertimos las variables a valores binarios (mantener como booleanos)\n",
        "#--------------------------------------------------------------------------------\n",
        "df_bin[ 'Glucose' ]                  = df_bin[ 'Glucose'                  ].apply(lambda x: 1 if x >= 140 else 0)\n",
        "df_bin[ 'BMI' ]                      = df_bin[ 'BMI'                      ].apply(lambda x: 1 if x >=  30 else 0)\n",
        "df_bin[ 'Age' ]                      = df_bin[ 'Age'                      ].apply(lambda x: 1 if x >=  50 else 0)\n",
        "df_bin[ 'DiabetesPedigreeFunction' ] = df_bin[ 'DiabetesPedigreeFunction' ].apply(lambda x: 1 if x >= 0.5 else 0)\n",
        "df_bin[ 'Outcome' ]                  = df_bin[ 'Outcome'                  ].apply(lambda x: 1 if x ==   1 else 0)\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 5. Nivelamos las clases usando SMOTE\n",
        "#   La técnica SMOTE ayuda a balancear el conjunto de datos para evitar que el modelo favorezca la clase mayoritaria\n",
        "#--------------------------------------------------------------------------------\n",
        "X = df_bin.drop( 'Outcome', axis = 1 )  # Eliminar la columna 'Outcome' de las características\n",
        "y = df_bin[ 'Outcome' ]                 # La columna 'Outcome' es el objetivo\n",
        "\n",
        "# Aplicamos SMOTE para sobremuestrear las clases\n",
        "smote = SMOTE( random_state = 42 )\n",
        "X_res, y_res = smote.fit_resample( X, y )\n",
        "\n",
        "# Creamos el DataFrame balanceado\n",
        "balanced_df = pd.DataFrame( X_res, columns = X.columns )\n",
        "balanced_df[ 'Outcome' ] = y_res\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 6. Generamos los conjuntos de elementos más frecuentes con Apriori\n",
        "#--------------------------------------------------------------------------------\n",
        "#   df            DataFrame con datos en formato binario\n",
        "#   min_support   Umbral mínimo de soporte para considerar conjuntos frecuentes\n",
        "#                   valores altos (0.3  a 0.5) generan pocas reglas pero muy relevantes\n",
        "#                   valores bajos (0.05 a 0.1) generan muchas reglas pero algunas poco significativas o con ruido\n",
        "#   use_colnames  Usa los nombres de las columnas en lugar de índices numéricos\n",
        "#   max_len       Tamaño máximo del conjunto de items generados.\n",
        "#   verbose       Muestra el progreso del algoritmo\n",
        "#   low_memory    Usa menos memoria (útil ante grandes conjuntos de datos)\n",
        "#   n_jobs        Número de procesadores en paralelo (-1 usa todos los disponibles) NO EXISTE EN mlxtend\n",
        "#--------------------------------------------------------------------------------\n",
        "conjuntoElementosFrecuentes = apriori(\n",
        "    balanced_df.round().astype( int ), # aseguramos valores binarios\n",
        "    min_support   = 0.1,\n",
        "    max_len       = 3,\n",
        "    verbose       = False,\n",
        "    use_colnames  = True\n",
        "    )\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 7. Generamos las reglas de asociación con association_rules en función de los conjuntos de elementos más frecuentes\n",
        "#--------------------------------------------------------------------------------\n",
        "#   frequent_itemsets   Contiene los conjuntos de elementos frecuentes y su soporte\n",
        "#   metric              Métricas que ayudan a evaluar la calidad de las reglas de asociación\n",
        "#                           confidence    probabilidad de que al ser cierto el Antecedente también lo sea el Consequente\n",
        "#                             cerca de 1    Fuerte relación entre Antecedente y Consecuente\n",
        "#                             valor bajo    Antecedente ocurre muchas veces sin que ocurra Consecuente\n",
        "#                           lift          probabilidad de que el Consecuente ocurra si el Antecedente es cierto, en comparación con la probabilidad general del Consecuente\n",
        "#                             > 1   Antecedente y Consecuente positivamente correlacionados\n",
        "#                             = 1   Antecedente y Consecuente son independientes\n",
        "#                             < 1   Antecedente reduce la probabilidad de Consecuente\n",
        "#                           support       mide la frecuencia con la que ocurre un conjunto de valores en el dataset\n",
        "#                             > 0.5   Regla más común y confiable\n",
        "#                             < 0.5   Puede ser un patrón raro pero valioso\n",
        "#                           leverage      mide la frecuencia de que Antecedente y Consecuente ocurran juntos en comparación de si fueran independientes\n",
        "#                             > 0   Antecedente y Consecuente aparecen juntos más de lo esperado\n",
        "#                             = 0   NO hay relación\n",
        "#                             < 0   Antecedente y Consecuente aparecen juntos menos de lo esperado\n",
        "#                           conviction    mide la dependencia del Consecuente frente al Antecedente comparado con que ocurra el Consecuente por sí solo\n",
        "#                             > 1           Relación fuerte en Antecedente y Consecuente\n",
        "#                             cercanos a 1  Relación débil\n",
        "#   min_threshold       Mantiene reglas con el nivel de la métrica especificado\n",
        "#--------------------------------------------------------------------------------\n",
        "#Reglas = association_rules(\n",
        "#    conjuntoElementosFrecuentes,\n",
        "#    metric        = \"confidence\",\n",
        "#    min_threshold = 0.2 )\n",
        "\n",
        "# Generamos ahora todas las reglas\n",
        "Reglas = association_rules( conjuntoElementosFrecuentes,\n",
        "   metric        = \"confidence\",\n",
        "   min_threshold = 0.6)\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 8. Filtramos por las reglas más importantes:\n",
        "#     confidence > 0.2\n",
        "#     lift       > 1.2\n",
        "#--------------------------------------------------------------------------------\n",
        "# Filtramos las reglas que cumplen con los valores de confidence y lift\n",
        "#reglasFiltradas = Reglas[ ( Reglas[ 'confidence' ] > 0.2 ) & ( Reglas[ 'lift' ] > 1.2 ) ]\n",
        "reglasFiltradas = Reglas[ ( Reglas[ 'confidence' ] > 0.80 ) ]\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 9. Filtramos ahora sólo por las columnas necesarias\n",
        "#--------------------------------------------------------------------------------\n",
        "reglasFiltradas = reglasFiltradas[ [ 'antecedents', 'consequents', 'support', 'confidence', 'lift' ] ].copy()\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 10. Convertimos antecedentes y consecuentes al formato deseado\n",
        "#--------------------------------------------------------------------------------\n",
        "reglasFiltradas[ 'antecedents' ] = reglasFiltradas[ 'antecedents' ].apply( lambda x: ', '.join( [ str( item ) for item in x ] ) )\n",
        "reglasFiltradas[ 'consequents' ] = reglasFiltradas[ 'consequents' ].apply( lambda x: ', '.join( [ str( item ) for item in x ] ) )\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 11. Calculamos el tamaño máximo de los valores en 'antecedents' y 'consequents'\n",
        "#--------------------------------------------------------------------------------\n",
        "maxAntecedents, maxConsequents = 16, 11\n",
        "if not reglasFiltradas.empty:\n",
        "    maxAntecedents = max( max( reglasFiltradas[ 'antecedents' ].apply( len ) ), 16 )\n",
        "    maxConsequents = max( max( reglasFiltradas[ 'consequents' ].apply( len ) ), 11 )\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 12. Filtramos las reglas donde \"Consequents\" sólo contenga \"Outcome\"\n",
        "#--------------------------------------------------------------------------------\n",
        "# Verificamos si \"Outcome\" es el único valor en \"Consequents\"\n",
        "reglasFiltradas[ 'only_outcome' ] = reglasFiltradas[ 'consequents' ].apply(lambda x: x == \"Outcome\" )\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 13. Ordenamos las reglas para que las que contienen sólo \"Outcome\" y en \"Consequents\" aparezcan al final\n",
        "#--------------------------------------------------------------------------------\n",
        "reglasFiltradasOrdenadas = reglasFiltradas.sort_values( by = 'only_outcome', ascending = True )\n",
        "\n",
        "# Eliminar la columna \"only_outcome\" ya que no es necesaria para la visualización\n",
        "reglasFiltradasOrdenadas = reglasFiltradasOrdenadas.drop( columns = [ 'only_outcome' ] )\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Paso 14. Mostramos los encabezados con tamaños dinámicos basados en el tamaño máximo de las listas\n",
        "#--------------------------------------------------------------------------------\n",
        "print( \"\\nReglas de asociación filtradas y ordenadas:\\n\" )\n",
        "print( f\"{'Antecedentes':<{maxAntecedents}}     {'Consecuencias':<{maxConsequents}}   {'Soporte':<10}{'Confianza':<12}{'Fuerza de asociación':<10}\")\n",
        "print( \"-\" * (maxAntecedents + maxConsequents + 50 ) )\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Paso 15. Imprimimos las reglas ordenadas con los valores correspondientes\n",
        "#-------------------------------------------------------------------------------\n",
        "if reglasFiltradasOrdenadas.empty:\n",
        "  print( \"No se ha encontrado ninguna regla que cumpla con los criterios.\" )\n",
        "else:\n",
        "  for index, row in reglasFiltradasOrdenadas.iterrows():\n",
        "    print( f\"( {row[ 'antecedents' ]:<{maxAntecedents}}) → ({row['consequents']:<{maxConsequents}})    {row['support']*100:.2f}%     {row['confidence']*100:.2f}%           {row['lift']:.2f}\" )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
