{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **PASOS A SEGUIR**\n",
        "---\n",
        "\n",
        "\n",
        "# **1. Instalación de las bibliotecas necesarias**\n",
        "\n",
        "\n",
        "**1.1. Operaciones con las Bibliotecas**\n",
        "\n",
        "a)   Instalación de una biblioteca\n",
        "\n",
        "\n",
        "      !pip install <biblioteca>\n",
        "      !pip install tensorflow\n",
        "b)   Actualización de una biblioteca\n",
        "\n",
        "\n",
        "      !pip install --upgrade <biblioteca>\n",
        "      !pip install --upgrade tensorflow\n",
        "c)  Eliminación de una bilioteca\n",
        "\n",
        "\n",
        "      !pip uninstall <biblioteca>\n",
        "      !pip uninstall tensorflow\n",
        "d)  Importación de una biblioteca en su totalidad\n",
        "\n",
        "\n",
        "      import <biblioteca>\n",
        "      import tensorflow\n",
        "e)  Importación de una biblioteca en su totalidad bajo un pseudónimo\n",
        "\n",
        "\n",
        "      import <biblioteca> as <pseudónimo>\n",
        "      import tensorflow as tf\n",
        "f)  Importación de una parte específica de una biblioteca\n",
        "\n",
        "\n",
        "      from <biblioteca> import  <parte>\n",
        "      from tensorflow import keras\n",
        "\n",
        "**1.2.  Búsqueda de datasets o de conjuntos de datos de una bibliteca**\n",
        "\n",
        "\n",
        "\n",
        "TensorFlow y Scikit-learn proporcionan varios conjuntos de datos que se pueden utilizar en las prácticas de Aprendizaje Automático y en el Análisis de datos.\n",
        "\n",
        "TensorFlow (TFDS) proporciona una amplia colección de conjuntos de datos listos\n",
        "para usar, abarcando un gran variedad de dominios (textos, imágenes, audio,...).\n",
        "Se puede acceder a los conjuntos de datos a través de tensorflow_datasets.\n",
        "\n",
        "Scikit-learn también incluyen varios conjuntos de datos que se pueden utiizar\n",
        "en las prácticas de Aprendizaje Automátido y en el Análisis de datos.\n",
        "Se puede acceder a los conjuntos de datos a través del módulo datasets.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "a)  Muestra los datasets de tensorflow\n",
        "\n",
        "\n",
        "      import tensorflow_datasets as tfds\n",
        "      datasetsTensorflow = tfds.list_builders()\n",
        "      print( \"Conjuntos de datos disponibles en TensorFlow:\" )\n",
        "      print( datasetsTensorflow )\n",
        "\n",
        "\n",
        "\n",
        "b)  Muestra los datasets de sklearn\n",
        "\n",
        "\n",
        "      from sklearn import datasets\n",
        "      datasetsSklearn = dir( datasets )\n",
        "      nombreDatasetsSklearn = [name[5:] for name in datasetsSklearn if name.startswith( 'load_' )]\n",
        "      print( \"Conjuntos de datos disponibles en Scikit-learn:\" )\n",
        "      print( \", \".join( nombreDatasetsSklearn ) )\n",
        "\n",
        "\n",
        "# **2. Preparación de los datos**\n",
        "---\n",
        "\n",
        "Algunos aspectos importantes a tener en cuenta en este paso son los siguientes:\n",
        "\n",
        "\n",
        "*   Modificar el orden de los datos: resulta, a veces, muy importante mezclar los datos para evitar que siempre se trabaje con el mismo orden, y así hacer que el orden de los datos NO sea determinante en su resultado.\n",
        "*   Visualizar los datos: también puede ser importante mostrar los datos para comprobar si hay correlaicones entre distintas características de los mismos.\n",
        "*   Reducir la dimensionalidad de los datos: simplifica su tratamiento.\n",
        "*   Balancear los datos: la idea es que sean más representativos y evitar, así, resultados falsos.\n",
        "*   Separar los datos en dos grupos (entrenamiento y evaluación): normalmente se realiza en una proporción de 80/20, pero puede variar.\n",
        "*   Preprocesar los datos normalizándolos: para evitar duplicados y hacer corrección de errores.\n",
        "\n",
        "\n",
        "**2.1. Importación del dataset o conjunto de datos**\n",
        "\n",
        "a)  Importamos el dataset desde un fichero\n",
        "\n",
        "\n",
        "    # Cargamos los datos desde la URL en un DataFrame de pandas\n",
        "    url       = 'iris.data'\n",
        "    columnas  = [ 'Sépalo_longitud', 'Sépalo_anchura', 'Pétalo_longitud', 'Pétalo_anchura', 'Especie' ]\n",
        "\n",
        "    # Cargar los datos desde la URL en un DataFrame de pandas con los nombres de columna especificados\n",
        "    IRIS = pd.read_csv( url, header = None, names = columnas )\n",
        "\n",
        "    # Quitamos el prefijo 'Iris-' de los valores en la columna 'Especie'\n",
        "    IRIS[ 'Especie' ] = IRIS[ 'Especie' ].str.replace( 'Iris-', '' )\n",
        "\n",
        "\n",
        "b)  Importamos el dataset desde una dirección web\n",
        "\n",
        "\n",
        "    # URL del conjunto de datos de iris en formato CSV, junto con el nombre de las columnas\n",
        "    url       = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
        "    columnas  = [ 'Sépalo_longitud', 'Sépalo_anchura', 'Pétalo_longitud', 'Pétalo_anchura', 'Especie' ]\n",
        "\n",
        "    # Cargamos los datos desde la URL en un DataFrame de pandas con los nombres de columna especificados\n",
        "    IRIS = pd.read_csv( url, header = None, names = columnas )\n",
        "\n",
        "    # Quitamos el prefijo 'Iris-' de los valores en la columna 'Especie'\n",
        "    IRIS[ 'Especie' ] = IRIS[ 'Especie' ].str.replace( 'Iris-', '' )\n",
        "\n",
        "\n",
        "c)  Importamos el dataset desde una librería de Scikit-learn o desde datasets estructurados a partir de plataformas como Kaggle\n",
        "\n",
        "\n",
        "    IRIS = datasets.load_iris()   # Cargando el dataset\n",
        "\n",
        "    # Convertimos el dataset a dataframe de Pandas\n",
        "    IRIS = pd.DataFrame(\n",
        "        data    = np.c_[  IRIS[ 'data'  ], IRIS[  'target'  ] ],\n",
        "        columns = IRIS[ 'feature_names'] + [ 'target' ]\n",
        "    )\n",
        "    IRIS.head( 5 )    # Muestra unas pocas filas del dataframe\n",
        "    \n",
        "    # Debido a que no hay una columna de nombres de especies, agregamos una columna más con nombres\n",
        "    # de diferentes especies correspondientes a sus valores numéricos.\n",
        "    # Esto ayuda a acceder a las diferentes clases usando sus nombres en lugar de números.\n",
        "\n",
        "    especies = []\n",
        "    for i in range( len( IRIS[ 'target' ] ) ):\n",
        "        if IRIS[ 'target' ][ i ] == 0:\n",
        "            especies.append( \"setosa\" )\n",
        "        elif IRIS[ 'target' ][ i ] == 1:\n",
        "            especies.append( 'versicolor' )\n",
        "        else:\n",
        "            especies.append( 'virginica' )\n",
        "    IRIS[ 'Especie' ] = especies\n",
        "\n",
        "    IRIS.head( 5 )      # Muestra las primeras filas del DataFrame para verificar los cambios\n",
        "    IRIS.describe()   # Muestra información estadística sencilla del dataset\n",
        "    IRIS.groupby( 'Especie' ).size()   # Muestra el número de elementos de cada clase: 50\n",
        "\n",
        "d)  Importamos el dataset desde una librería de Scikit-learn o desde\n",
        "\n",
        "\n",
        "    from sklearn.datasets import load_iris\n",
        "    IRIS = load_iris()   # Cargando el dataset\n",
        "    IRIS.keys()       # Muestra las llaves del dataset, algunas servirán para acceder a datos específicos\n",
        "\n",
        "    # Convertimos el dataset a dataframe de Pandas\n",
        "    IRIS = pd.DataFrame(\n",
        "    data    = np.c_[  IRIS[ 'data'  ], IRIS[  'target'  ] ],\n",
        "    columns = IRIS[ 'feature_names'] + [ 'target' ]\n",
        "    )\n",
        "    IRIS.head( 5 )    # Muestra unas pocas filas del dataframe\n",
        "\n",
        "    # Debido a que no hay una columna de nombres de especies, agreguemos una columna\n",
        "    # más con nombres de diferentes especies correspondientes a sus valores numéricos.\n",
        "    # Esto ayuda a acceder a las diferentes clases usando sus nombres en lugar de números.\n",
        "\n",
        "    especies = []\n",
        "    for i in range( len( IRIS[ 'target' ] ) ):\n",
        "        if IRIS[ 'target' ][ i ] == 0:\n",
        "            especies.append( \"setosa\" )\n",
        "        elif IRIS[ 'target' ][ i ] == 1:\n",
        "            especies.append( 'versicolor' )\n",
        "        else:\n",
        "            especies.append( 'virginica' )\n",
        "    \n",
        "    IRIS[ 'Especie' ] = especies\n",
        "\n",
        "e)  Importamos el dataset de Scikit-learn\n",
        "\n",
        "\n",
        "    from sklearn.datasets import load_iris\n",
        "\n",
        "    # Cargamos el dataset Iris\n",
        "    iris = load_iris()\n",
        "    \n",
        "    # Creamos un DataFrame de pandas con los datos\n",
        "    IRIS = pd.DataFrame( data = iris.data, columns = iris.feature_names )\n",
        "    \n",
        "    # Agregar la columna de especies\n",
        "    IRIS[ 'Especie' ] = iris.target_names[ iris.target ]\n",
        "\n",
        "f)  Importamos los datos en dos lista: entrenamiento y prueba\n",
        "\n",
        "\n",
        "    ( xTrain, yTrain ), ( xTest, yTest ) = load_iris()\n",
        "\n",
        "g)  Generamos los datos de forma aleatoria\n",
        "\n",
        "\n",
        "    Muestras = 150  # 150 muestras para cada clase (50 para setosa, 50 para versicolor, 50 para virginica)\n",
        "\n",
        "    # Generar datos sintéticos para cada clase\n",
        "    np.random.seed( 42 )  # Fijar la semilla para reproducibilidad\n",
        "    setosa      = np.random.normal( loc = [ 5, 3.5, 1.4, 0.2 ], scale = [ 2.5, 1.5, 0.5, 0.5 ], size =( Muestras//3, 4))\n",
        "    versicolor  = np.random.normal( loc = [ 6, 2.8, 4.5, 1.5 ], scale = [ 2.5, 1.5, 0.5, 0.3 ], size =( Muestras//3, 4))\n",
        "    virginica   = np.random.normal( loc = [ 6.5, 3, 5.5, 2   ], scale = [ 2.5, 1.5, 1.0, 0.5 ], size =( Muestras//3, 4))\n",
        "\n",
        "    # Concatenar los datos de las tres clases\n",
        "    clases = np.concatenate( [ setosa, versicolor, virginica ] )\n",
        "\n",
        "    # Crear etiquetas para las clases\n",
        "    etiquetas = np.array(['setosa'] * (Muestras//3) + ['versicolor'] * (Muestras//3) + ['virginica'] * (Muestras//3))\n",
        "\n",
        "    # Convertir datos_sinteticos y etiquetas en un DataFrame de pandas\n",
        "    IRIS = pd.DataFrame( clases, columns = ['Sépalo_longitud', 'Sépalo_anchura', 'Pétalo_longitud', 'Pétalo_anchura'])\n",
        "    IRIS[ 'Especie' ] = etiquetas\n",
        "\n",
        "h)  Generamos los datos de forma aleatoria y con funciones def asociadas\n",
        "\n",
        "\n",
        "    def asignarDatosAleatorios( muestras, valoresMedios, Desviaciones ):\n",
        "        numCaracteristicas = len( valoresMedios )\n",
        "        datos = np.random.normal( loc = valoresMedios, scale = Desviaciones, size = ( muestras, numCaracteristicas))\n",
        "        return datos\n",
        "\n",
        "    def asignarDatosSetosa( muestras ):\n",
        "        valoresMedios = [ 5.0, 3.5, 1.4, 0.2 ]\n",
        "        desviaciones  = [ 2.5, 1.5, 0.5, 0.5 ]\n",
        "        datosSetosa = asignarDatosAleatorios( muestras, valoresMedios, desviaciones )\n",
        "        return datosSetosa\n",
        "\n",
        "    def asignarDatosVersicolor( muestras ):\n",
        "        valoresMedios = [ 6.0, 2.8, 4.5, 1.5 ]\n",
        "        desviaciones  = [ 2.5, 1.5, 0.5, 0.3 ]\n",
        "        datosVersicolor = asignarDatosAleatorios( muestras, valoresMedios, desviaciones )\n",
        "        return datosVersicolor\n",
        "    \n",
        "    def asignarDatosVirginica( muestras ):\n",
        "        valoresMedios = [ 6.5, 3.0, 5.5, 2.0 ]\n",
        "        desviaciones  = [ 2.5, 1.5, 1.0, 0.5 ]\n",
        "        datosSetosa = asignarDatosAleatorios( muestras, valoresMedios, desviaciones )\n",
        "        return datosSetosa\n",
        "    \n",
        "    muestras = 50\n",
        "    datosSetosa     = asignarDatosSetosa(     muestras )\n",
        "    datosVersicolor = asignarDatosVersicolor( muestras )\n",
        "    datosVirginica  = asignarDatosVirginica(  muestras )\n",
        "\n",
        "    clases = np.concatenate( [ datosSetosa, datosVersicolor, datosVirginica ] )\n",
        "\n",
        "    # Crear etiquetas para las clases\n",
        "    etiquetas = np.repeat( [ 'setosa', 'versicolor', 'virginica' ], muestras )\n",
        "\n",
        "    # Convertir datos_sinteticos y etiquetas en un DataFrame de pandas\n",
        "    IRIS = pd.DataFrame( clases, columns = [ 'Sépalo_longitud', 'Sépalo_anchura', 'Pétalo_longitud', 'Pétalo_anchura' ] )\n",
        "    IRIS[ 'Especie' ] = etiquetas\n",
        "\n",
        "    IRIS.head(5)      # Muestra las primeras filas del DataFrame para verificar los cambios\n",
        "    IRIS.describe()   # Muestra información estadística sencilla del dataset\n",
        "    IRIS.groupby( 'Especie' ).size()   # Muestra el número de elementos de cada clase: 50\n",
        "\n",
        "i)  Descargamos datos de un Dataset, extraemos imágenes y texto, los agrupamos en tablas (imágenes y etiquetas), y los convertimos en un dataset de Tensorflow\n",
        "\n",
        "\n",
        "    # fashion_mnist, mnist, cifar10, cifar100, symmetric_solids, citrus_leaves, food101\n",
        "    # stanford_dogs, squad, beans\n",
        "    NOMBRE_DATASET = input( \"Nombre o ruta del dataset: \")\n",
        "\n",
        "    dataset, metadatos = tfds.load(NOMBRE_DATASET, as_supervised=True, with_info=True)\n",
        "    if( 'train' in dataset):\n",
        "        train_data = dataset[ 'train' ]\n",
        "        DATOS_TOTALES = train_data\n",
        "        if( 'test' in dataset ):\n",
        "            test_data = dataset[ 'test' ]\n",
        "            DATOS_TOTALES = train_data.concatenate( test_data )\n",
        "        else:\n",
        "            DATOS_TOTALES = dataset\n",
        "\n",
        "    # Obtenemos las imágenes y las etiquetas de DATOS_TOTALES\n",
        "    imagenes = []\n",
        "    etiquetas = []\n",
        "    for imagen, etiqueta in DATOS_TOTALES:\n",
        "        imagenes.append( imagen.numpy() )\n",
        "        etiquetas.append( etiqueta.numpy() )\n",
        "\n",
        "    imagenes = np.array(imagenes)\n",
        "    etiquetas = np.array(etiquetas)\n",
        "\n",
        "    # Dividimos los datos en conjuntos de entrenamiento y prueba\n",
        "    x_train, x_test, y_train, y_test = train_test_split(imagenes, etiquetas, test_size=0.35, random_state=42)\n",
        "\n",
        "    # Convertimos los conjuntos de datos en objetos tf.data.Dataset\n",
        "    datos_entrenamiento = tf.data.Dataset.from_tensor_slices( ( x_train, y_train ) )\n",
        "    datos_pruebas       = tf.data.Dataset.from_tensor_slices( ( x_test, y_test ) )\n",
        "\n",
        "    # Agregamos los datos a caché por razones de eficiencia en el entrenamiento\n",
        "    # Aplicamos el método cache() a los conjuntos de datos\n",
        "    datos_entrenamiento = datos_entrenamiento.cache()\n",
        "    datos_pruebas       = datos_pruebas.cache()\n",
        "\n",
        "\n",
        "**2.2.  División de los datos en entrenamiento y prueba**\n",
        "\n",
        "\n",
        "    train_test_split( *arrays, test_size = None, train_size = None, random_state = None, shuffle = True, stratify = None )\n",
        "\n",
        "*    **arrays**:      arreglos que se dividirán. Puede ser una lista o una tupla de arreglos NumPy u otros objetos similares.\n",
        "*   **test_size**:    tamaño del conjunto de datos de prueba. Puede ser un número entre 0.0 y 1.0 (proporción del conjunto de datos total), o un entero (número exacto de muestras para el conjunto de prueba).\n",
        "*    **train_size**:   tamaño del conjunto de datos de entrenamiento de manera similar a test_size.\n",
        "\n",
        "*   **random_state**: controla la aleatorización aplicada antes de dividir los datos. Si se proporciona un entero, fija la semilla aleatoria para garantizar la reproducibilidad de la división.\n",
        "    \n",
        "*    **shuffle**:      Indica si los datos deben mezclarse antes de dividirlos. Por defecto es True, lo que significa que los datos se mezclarán aleatoriamente antes de dividirlos.\n",
        "    \n",
        "*    **stratify**:     Opcionalmente, permite realizar una división estratificada basada en las etiquetas. Si se proporcionan etiquetas, la división garantizará que las proporciones de las clases sean las mismas en los conjuntos de entrenamiento y prueba.\n",
        "\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    (xTrain, xTest), (yTrain, yTest)  = train_test_split( iris, test_size = 0.2, random_state = 42 )\n",
        "\n",
        "      iris                Conjunto de datos completo, debe contener características y etiquetas.\n",
        "      test_size = 0.2     Tamaño de los datos de prueba del 20%\n",
        "      random_state = 42   Semilla aleatoria en 42 para garantizar la reproducibilidad de los resultados\n",
        "\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    xTrain, xTest, yTrain, yTest      = train_test_split( iris.data, iris.target, test_size = 0.2, random_state = 42 )\n",
        "\n",
        "      iris.data           Características o atributos de las muestras en forma de matriz NumPy: longitud del sépalo, anchura del sépalo, longitud del pétalo y anchura del pétalo\n",
        "      iris.target         Etiquetas de las muestras en forma de matrizNumpy: setosa, versicolor y virginica, codificadas como enteros 0, 1 y 2 respectivamente\n",
        "      test_size = 0.2     Tamaño de los datos de prueba del 20%\n",
        "      random_state = 42   Semilla aleatoria en 42 para garantizar la reproducibilidad de los resultados\n",
        "\n",
        "\n",
        "**2.3.  Obtención de las dimensiones del array**\n",
        "\n",
        "\n",
        "  **Función shape**: es un atributo de los objetos de tipo array proporcionados por bibliotecas como NumPy y que devuelve las dimensiones del arreglo o matriz.\n",
        "       En una matriz bidimensional, devuelve filas y columnas.\n",
        "\n",
        "       datos = np.array( [ [ 1, 2, 3 ], [ 4, 5, 6 ] ] )\n",
        "       print( datos.shape )          muestra (2, 3)\n",
        "       print( datos.shape[ 0 ] )     muestra el número de filas de la tabla\n",
        "       print( datos.shape[ 1 ] )     muestra el número de columnas de la tabla\n",
        "       dimension = datos.shape\n",
        "\n",
        "\n",
        "**2.4.  Redimensionamiento de arreglos (o imágenes)**\n",
        "\n",
        "**Método reshape**: se utiliza para cambiar la forma (shape) de un arreglo o matriz multidimensional.\n",
        "\n",
        "Es importante tener en cuenta que el producto de las dimensiones debe ser igual al número total de elementos del arreglo o matriz.\n",
        "\n",
        "    reshape( array, newshape, order = 'C' )\n",
        "*   **array**:     arreglo o tabla que se va a remodelar\n",
        "*   **newshape**:  nueva forma deseada de la tabla\n",
        "*   **order**:     orden de lectura de los elementos: C (fila principal), F (Fortram, columna principal)\n",
        "\n",
        "\n",
        "      datos = np.array( [ 1, 2, 3, 4, 5, 6 ] )\n",
        "      datos = datos.reshape( datos.shape[ 0 ], ( 2, 3 ) )\n",
        "      xTrain = xTrain.reshape( xTrain.shape[ 0 ], 30, 30, 1 )\n",
        "\n",
        "\n",
        "**2.5.  Normalización de los valores de una variable a valores comprendidos entre 0 y 1**\n",
        "\n",
        "Convertimos los valores de una matriz de rango 0 a 255 a valores del rango 0 ó 1.\n",
        "\n",
        "La idea es simplificar las operaciones que se realizarán sobre esos datos.\n",
        "\n",
        "Si trabajamos con imágenes, esto significa convertir las imágenes de color a blanco y negro.\n",
        "\n",
        "    datos = datos / 255.0\n",
        "    xTrain = xTrain / 255.0\n",
        "\n",
        "\n",
        "**2.6.  Conversión de etiquetas de clase a vector binario con un solo 1 (one-hot encoding)**\n",
        "\n",
        "Técnica utilizada para representar variables categóricas como vectores binarios, con un tamaño igual al número de categorías, y dicho vector se compondrá de 0s menos la posicion de la categoría representada, que será 1.\n",
        "\n",
        "    to_categorical( etiquetas, num_classes = None, dtype = 'float32' )\n",
        "*   **etiquetas**:   arreglo o matriz de etiquetas de clase\n",
        "*   **num_classes**: número total de clases de datos, si no hay se infiere automáticamente\n",
        "*   **dtype**:       tipo de dato de la matriz de salida\n",
        "\n",
        "\n",
        "    from tensorflow.keras.utils import to_categorical\n",
        "    datos = to_categorical( datos )\n",
        "    xTrain = to_categorical( xTrain )\n",
        "\n",
        "\n",
        "**2.7.  Conversión de los datos a otra unidad**\n",
        "\n",
        "**Función astype**: sirve para cambiar el tipo de datos de todo un arreglo Numpy a otro tipo de datos.\n",
        "\n",
        "    datos = np.array( [ 1, 2, 3, 4 ] )\n",
        "    datos = datos.astype( 'float' )\n",
        "    xTrain = xTrain.astype( 'float32' )\n",
        "\n",
        "\n",
        "**2.8. Convertir listas de datos a Datasets**\n",
        "\n",
        "Crea un objeto Dataset a partir de distintos tensores que se pasan por parámetros, agrupados entre paréntesis y separados por comas.\n",
        "\n",
        "    datosEntrenamiento = tf.data.Dataset.from_tensor_slices( ( x_train, y_train ) )\n",
        "    datosEntrenamiento = from_tensor_slices( ( x_train, y_train ) )\n",
        "\n",
        "\n",
        "**2.9. Agregar datos a caché por razones de eficiencia en el entrenamiento**\n",
        "\n",
        "Se suele incluir cache() después de la operación de reordenamiento, y así, los elementos del conjunto de datos se almacenarán en caché después de haber sido mezclados y repetidos. Esto puede ser útil si el preprocesamiento o la generación de los datos es costoso en términos computacionales, ya que evitará que este proceso se repita cada vez que se itere sobre el conjunto de datos durante el entrenamiento.\n",
        "\n",
        "    datosEntrenamiento = datosEntrenamiento.cache()\n",
        "\n",
        "**2.10. Reordenar los datos**\n",
        "\n",
        "**Función shuffle**: reordena aleatoriamente los elementos de un conjunto de datos. Esta función es comúnmente utilizada en el preprocesamiento de datos para introducir aleatoriedad en el orden de los ejemplos de entrenamiento, lo cual puede ser beneficioso para mejorar la eficacia del entrenamiento de modelos de aprendizaje automático, especialmente en casos donde existe una dependencia en el orden de los datos.\n",
        "\n",
        "    datosEntrenamiento = datosEntrenamiento.shuffle( buffer_size )\n",
        "\n",
        "El parámetro buffer_size determina el tamaño del búfer de mezcla.\n",
        "\n",
        "    shuffle( buffer_size = len( x_train ) )\n",
        "\n",
        "\n",
        "\n",
        "# **3. Elección del modelo**\n",
        "---\n",
        "\n",
        "La elección del modelo puede ayudar en la confección de un mejor o peor sistema, pero se puede elegir en función del objetivo buscado:\n",
        "\n",
        "\n",
        "*   Regresión logística: aplicaciones de predicción.\n",
        "*   Redes totalmente conectadas: aplicaciones de clasificación.\n",
        "*   Redes neuronales convolucionales: aplicaciones de asociadas al procesamiento de imágenes.\n",
        "*   Redes neuronales recurrentes: aplicaciones de reconocimiento de voz.\n",
        "*   Bosque aleatorio: aplicaciones de reconocimiento de fraudes.\n",
        "*   Aprendizaje reforzado: aplicaciones para enseñar a jugar.\n",
        "*   Modelos generativos: aplicaciones para crear imágenes.\n",
        "*   Modelo K-means: aplicaciones para segmentar datos sin etiquetar.\n",
        "*   Modelo vecino más próximo: aplicaciones para recomendar por similitud o cercanía.\n",
        "*   Clasificadores Bayesianos: aplicaciones de clasificación de contenidos, emails.\n",
        "\n",
        "\n",
        "\n",
        "Paso crucial en el proceso de construcción de una aplicación de Machine Learning. Los parámetros que se configuran son los siguientes:\n",
        "*   Número de capas y unidades\n",
        "*   Función de activación\n",
        "*   Optimizador y función de pérdida\n",
        "*   Reducir o eliminar problemas de overfitting (o underfitting).\n",
        "*   Tasa de aprendizaje.\n",
        "*   Máximo error permitido.\n",
        "---\n",
        "\n",
        "\n",
        "* CAPA DENSA:   Dense( neuronas, kernel_size=(n,n), activation='tipo', input_shape=(m,n) )\n",
        "  *   Dense( neuronas, activation='relu', input_shape=(m,n) )  -> capa de entrada al modelo\n",
        "  *   Dense( neuronas, activation='relu')                     -> capa intermedia del modelo\n",
        "  *   Dense( neuronas, activation='sigmoid')                  -> capa de selección del modelo\n",
        "\n",
        "* CAPA CONVOLUTIONAL: Conv2D( neuronas, kernel_size=(n,n), activation='tipo'', input_shape=(m,n) )\n",
        "  *   Conv2D( neuronas, kernel_size=(n,n), activation='relu', input_shape=(m,n))  -> capa de entrada al modelo\n",
        "  *   Conv2D( neuronas, kernel_size=(n,n), activation='relu')                     -> capa intermedia del modelo\n",
        "\n",
        "* CAPA DE AGRUPACIÓN (POOLING): MaxPooling2D( pool_size=(n,n)  )\n",
        "  *   MaxPooling2D( (n,n) )\n",
        "\n",
        "* CAPA RECURRENTE: (LSTM, GRU)\n",
        "  *   LSTM( unidades )\n",
        "\n",
        "* CAPA DE INCRUSTACIÓN: mapea valores discretos (input_dim) en vectores de números reales (output_dim)\n",
        "  *   Embedding(input_dim=n, output_dim=m)\n",
        "\n",
        "* CAPA DE REGULACIÓN (DROPOUT): anula un % aleatorio de unidades de entrada\n",
        "  *   Dropout( valor )\n",
        "\n",
        "* CAPA DE NORMALIZACIÓN POR LOTES: normaliza las actividades de una capa anterior, acelerando el entrenamiento y mejorando la estabilidad del modelo\n",
        "  *   BatchNormalization()\n",
        "\n",
        "* CAPA DE ACTIVACIÓN: aplica una función de activación a las salidas de una capa anterior: relu, sigmoid, tanh, ...\n",
        "  *   activation = 'función de activación'\n",
        "\n",
        "* CAPA DE CONCATENACIÓN: une varias salidas de capas precedentes a lo largo de eje\n",
        "  *   Concatenate()\n",
        "\n",
        "\n",
        "\n",
        "*  OPTIMIZER: algoritmo de optimización durante el entrenamiento\n",
        "   *  Con ritmo de aprendizaje adaptativo:  'adam', 'adamax', 'nadam', 'adamax'\n",
        "   *  Con descenso de gradiente:  'sgd', 'adagrad', 'adadelta'\n",
        "   *  Basado en algoritmos avanzados: 'rmsprop', 'amsgrad', 'adagrad_da'\n",
        "\n",
        "*  LOSS: función de pérdida para calcular diferencia entre valores de predicción y los reales\n",
        "  * Clasificación binaria: 'binary_crossentropy', 'log_loss'\n",
        "  * Clasificación multiclase: 'categorical_crossentropy', 'sparse_categorical_crossentropy'\n",
        "  * Clasificación regresión: 'mean_squared_error', 'mean_absolute_error', 'mean_squared_logarithmic_error'\n",
        "\n",
        "*  METRICS: métrica para evaluar el rendimiento del modelo\n",
        "  * Clasificación binaria o multiclase: 'accuracy', 'precision', 'recall', 'fi_score'\n",
        "  * Clasificación de regresión: 'mean_absolute_error', 'r_squared'\n",
        "\n",
        "\n",
        "# **6. Entrenamiento del modelo**\n",
        "---\n",
        "\n",
        "En este paso se utiliza un set de datos de entrenamiento para ejecutar el modelo y observar una mejora incremental en la predicción.\n",
        "\n",
        "Resulta muy importante inicializar los pesos del modelo de forma aleatoria, porque afectan a las relaciones entre las entradas y las salidas, pero con el paso del tiempo, se irán ajustanto automáticamente según el algoritmo elegido y el número de entranamientos.\n",
        "\n",
        "Se debe observar los resultados obtenidos y corregirlos, y volver a iterar hasta alcanzar unos resultados deseables.\n",
        "\n",
        "\n",
        "\n",
        "# **7. Evaluación del modelo**\n",
        "---\n",
        "Se debe comprobar el modelo creado con un grupo de datos de evaluación, con entradas desconocidas por el sistema para verificar la precisión de dicho modelo.\n",
        "\n",
        "Si la exactitud no resulta superior al 50%, el modelo no será muy útil para la toma de decisiones.\n",
        "\n",
        "Si el modelo alcanza un 90% o más, tendremos una buena confianza en los resultados proporcionados por el modelo.\n",
        "\n",
        "\n",
        "# **8. Predicción o inferencia del modelo**\n",
        "---"
      ],
      "metadata": {
        "id": "6j9SBiCxtkL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La inferencia es el proceso de generación de predicciones del modelo para los nuevos datos que no se usan durante el entrenamiento."
      ],
      "metadata": {
        "id": "XehxWxF3tlGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Instalación de las bibliotecas necesarias**\n",
        "---"
      ],
      "metadata": {
        "id": "ZiQew8N0ps8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-----------------------------------------------------------------------------------------------------\n",
        "# 1.  Instalación de las bibliotecas necesarias\n",
        "#-----------------------------------------------------------------------------------------------------\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "\n",
        "from PIL import Image\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "sBqn6zNoptE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Preparación de los datos**\n",
        "---"
      ],
      "metadata": {
        "id": "iY-benQlptOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------------------------------------------\n",
        "# 2. Preparación de los datos\n",
        "#------------------------------------------------------------------------\n",
        "#    Descargamos un conjunto de datos con parte 'train' y 'test'\n",
        "#------------------------------------------------------------------------\n",
        "# datos, metadatos = tfds.load( '', as_supervised = True, with_info = True )\n",
        "#------------------------------------------------------------------------\n",
        "# fashion_mnist, mnist, cifar10, cifar100, symmetric_solids, citrus_leaves, food101\n",
        "# stanford_dogs, squad, beans\n",
        "#------------------------------------------------------------------------\n",
        "TAMAÑO_PRUEBA = 0.35\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        NOMBRE_DATASET = input( \"Nombre o ruta del dataset: \" )\n",
        "        dataset, metadatos = tfds.load( NOMBRE_DATASET, as_supervised = True, with_info = True )\n",
        "\n",
        "        if 'train' in dataset:\n",
        "            train_data = dataset[ 'train' ]\n",
        "            DATOS_TOTALES = train_data\n",
        "            if 'test' in dataset:\n",
        "                test_data = dataset[ 'test' ]\n",
        "                DATOS_TOTALES = train_data.concatenate( test_data )\n",
        "        else:\n",
        "            DATOS_TOTALES = dataset\n",
        "\n",
        "        break  # Sale del bucle si la carga del dataset es exitosa\n",
        "    except Exception as e:\n",
        "        print(f\"No se pudo cargar el dataset. Error: {e}\")\n",
        "\n",
        "# Obtener las imágenes y las etiquetas de DATOS_TOTALES\n",
        "imagenes = []\n",
        "etiquetas = []\n",
        "for imagen, etiqueta in DATOS_TOTALES:\n",
        "    imagenes.append( imagen.numpy() )\n",
        "    etiquetas.append( etiqueta.numpy() )\n",
        "\n",
        "imagenes = np.array( imagenes )\n",
        "etiquetas = np.array( etiquetas )\n",
        "\n",
        "# Redimensionar todas las imágenes a la dimensión de la primera imagen\n",
        "\n",
        "# Dividimos los datos en conjuntos de entrenamiento y prueba\n",
        "x_train, x_test, y_train, y_test = train_test_split( imagenes, etiquetas, test_size = TAMAÑO_PRUEBA, random_state = 42 )\n",
        "\n",
        "# Convertimos los conjuntos de datos en objetos tf.data.Dataset\n",
        "datosEntrenamiento = tf.data.Dataset.from_tensor_slices( ( x_train, y_train ) )\n",
        "datosPruebas       = tf.data.Dataset.from_tensor_slices( ( x_test, y_test ) )\n",
        "\n",
        "# Aplicamos el método cache() a los conjuntos de datos por cuestiones de eficiencia\n",
        "datosEntrenamiento = datosEntrenamiento.cache()\n",
        "datosPruebas       = datosPruebas.cache()"
      ],
      "metadata": {
        "id": "ELkNHIOJptWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Elección del modelo**\n",
        "---"
      ],
      "metadata": {
        "id": "T7p9dR2SptfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------------------------------------------\n",
        "# 3. Elección del modelo\n",
        "#------------------------------------------------------------------------\n",
        "modelo = tf.keras.Sequential()"
      ],
      "metadata": {
        "id": "fI89SiuTptmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Configuración de los parámetros del modelo**\n",
        "---"
      ],
      "metadata": {
        "id": "QxXsnkh-ptut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------------------------------------------\n",
        "# 4. Configuración de los parámetros del modelo:\n",
        "#------------------------------------------------------------------------\n",
        "nombres_clases  = metadatos.features[ 'label' ].names\n",
        "#INPUT_SHAPE     = metadatos.features[ 'image' ].shape\n",
        "CLASES_ELEMENTOS = len( nombres_clases )\n",
        "INPUT_SHAPE = imagenes[ 0 ].shape\n",
        "TAMAÑO_KERNEL = ( 3, 3 )\n",
        "TAMAÑO_POOLING = ( 2, 2 )\n",
        "TAMAÑO_DROPOUT = 0.25\n",
        "ACTIVACION = 'relu'\n",
        "\n",
        "modelo.add( Conv2D( 128, kernel_size = TAMAÑO_KERNEL, activation = ACTIVACION, input_shape = INPUT_SHAPE ) )\n",
        "modelo.add( MaxPooling2D( TAMAÑO_POOLING ) )\n",
        "modelo.add( Dropout( TAMAÑO_DROPOUT ) )\n",
        "modelo.add( BatchNormalization() )\n",
        "\n",
        "modelo.add( Conv2D( 64, TAMAÑO_KERNEL, activation = ACTIVACION ) )\n",
        "modelo.add( MaxPooling2D( TAMAÑO_POOLING ) )\n",
        "modelo.add( Dropout( TAMAÑO_DROPOUT ) )\n",
        "modelo.add( BatchNormalization() )\n",
        "\n",
        "modelo.add( Conv2D( 32, TAMAÑO_KERNEL, activation = ACTIVACION ) )\n",
        "modelo.add( MaxPooling2D( TAMAÑO_POOLING ) )\n",
        "modelo.add( Dropout( TAMAÑO_DROPOUT ) )\n",
        "modelo.add( BatchNormalization() )\n",
        "\n",
        "modelo.add( Flatten() )\n",
        "modelo.add( Dense( 40, activation = ACTIVACION ) )\n",
        "modelo.add( Dense( CLASES_ELEMENTOS, activation = 'softmax' ) )"
      ],
      "metadata": {
        "id": "EGXxPE7dpt1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Compilación del modelo:** compile( optimizer, loss, metrics )\n",
        "---"
      ],
      "metadata": {
        "id": "U49UjasEpt9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------------------------------------------\n",
        "# 5. Compilación del modelo\n",
        "#------------------------------------------------------------------------\n",
        "modelo.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss      = 'sparse_categorical_crossentropy',\n",
        "    metrics   = [ 'accuracy' ]\n",
        ")"
      ],
      "metadata": {
        "id": "3SpViW0KpuEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Entrenamiento del modelo**\n",
        "---"
      ],
      "metadata": {
        "id": "Gd7hu8nvpuMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------------------------------------------\n",
        "# 6. Entrenamiento del modelo\n",
        "#------------------------------------------------------------------------\n",
        "# DATOS NECESARIOS PARA EL ENTRENAMIENTO\n",
        "#------------------------------------------------------------------------\n",
        "NUM_ENTRENAMIENTO = len( datosEntrenamiento )\n",
        "NUM_PRUEBAS       = len( datosPruebas )\n",
        "LOTE              =  32\n",
        "TAMANO_LOTE       = math.ceil( NUM_ENTRENAMIENTO / LOTE )\n",
        "ACIERTOS          = 0.50\n",
        "NOM_FICHERO       = NOMBRE_DATASET + '.keras'\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "# SHUFFLE HACE QUE LOS DATOS ESTÉN MEZCLADOS DE FORMA ALEATORIA PARA QUE\n",
        "# LA RED NO SE APRENDA EL ORDEN\n",
        "#------------------------------------------------------------------------\n",
        "datosEntrenamiento = datosEntrenamiento.shuffle( NUM_ENTRENAMIENTO ).batch( LOTE )\n",
        "datosPruebas       = datosPruebas.batch( LOTE )\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "# ENTRENAMOS EL MODELO\n",
        "#------------------------------------------------------------------------\n",
        "historial = modelo.fit( datosEntrenamiento, epochs = 1, steps_per_epoch = TAMANO_LOTE )\n",
        "modelo.save( NOM_FICHERO )\n",
        "while( historial.history[ 'accuracy' ][ 0 ] < ACIERTOS ):\n",
        "  datosEntrenamiento.shuffle( NUM_ENTRENAMIENTO ).batch( LOTE )\n",
        "  historial = modelo.fit( datosEntrenamiento, epochs = 1, steps_per_epoch = TAMANO_LOTE )\n",
        "  modelo.save( NOM_FICHERO )"
      ],
      "metadata": {
        "id": "cB5q8F2wpuTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Evaluación del modelo**\n",
        "---"
      ],
      "metadata": {
        "id": "CozYKFNYpube"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------------------------------------------\n",
        "# 7. Evaluación del modelo\n",
        "#------------------------------------------------------------------------\n",
        "evaluacion = modelo.evaluate( datosPruebas, verbose = 0 )\n",
        "print( \"Resultados de la evaluación:\" )\n",
        "print( \"----------------------------\" )\n",
        "print( f\"Pérdida: {evaluacion[ 0 ]   * 100:.2f}%\" )\n",
        "print( f\"Precisión: {evaluacion[ 1 ] * 100:.2f}%\" )"
      ],
      "metadata": {
        "id": "splixv8OpukO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. Predicción o inferencia del modelo**\n",
        "---"
      ],
      "metadata": {
        "id": "0Z3IRmbUpusm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------------------------------------------\n",
        "# 8. Predicción o inferencia\n",
        "#------------------------------------------------------------------------\n",
        "def trazar_imagen( i, array_predicciones, etiquetas_reales, imagenes):\n",
        "  array_predicciones, etiqueta_real, imagen = array_predicciones[i], etiquetas_reales[i], imagenes[i]\n",
        "  plt.grid( False )\n",
        "  plt.xticks( [] )\n",
        "  plt.yticks( [] )\n",
        "  plt.imshow( imagen[ ..., 0 ], cmap = plt.cm.binary )\n",
        "  etiqueta_prediccion = np.argmax( array_predicciones )\n",
        "  color = 'blue'\n",
        "  if( etiqueta_prediccion != etiqueta_real):\n",
        "    color = 'red'\n",
        "  plt.title( 'Clase {}'.format( nombres_clases[etiqueta_real] ) )\n",
        "  plt.xlabel( \"{} ({:2.0f}%)\".format(nombres_clases[etiqueta_prediccion],100*np.max(array_predicciones)),color=color)\n",
        "\n",
        "filas = 2\n",
        "columnas = 5\n",
        "numImagenes = filas * columnas\n",
        "\n",
        "for imagenesPrueba, etiquetasPrueba in datosPruebas.shuffle( len( datosPruebas ) ).take( numImagenes ):\n",
        "  imagenesPrueba = imagenesPrueba.numpy()\n",
        "  etiquetasPrueba = etiquetasPrueba.numpy()\n",
        "\n",
        "predicciones = modelo.predict( imagenesPrueba, verbose = 0 )\n",
        "\n",
        "\n",
        "#plt.figure( figsize = ( 14, 7 ) )\n",
        "plt.figure( figsize = ( 10, 6 ) )\n",
        "\n",
        "plt.subplots_adjust( hspace = 0, wspace = 0 )\n",
        "\n",
        "for i, _ in enumerate( range( numImagenes ) ):\n",
        "    plt.subplot( filas, columnas, i + 1 )\n",
        "    trazar_imagen( i, predicciones, etiquetasPrueba, imagenesPrueba )"
      ],
      "metadata": {
        "id": "u4A-eMPlpu0N"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}