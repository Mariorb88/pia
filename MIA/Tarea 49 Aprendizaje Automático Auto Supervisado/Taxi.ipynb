{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-RD9H2sHK3f",
        "outputId": "51410674-3730-4102-c618-53326530cc7d"
      },
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# INSTALAMOS LA LIBRERÍA gymnasium\n",
        "#-------------------------------------------------------------------------\n",
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaO-Cx0XLgQU"
      },
      "source": [
        "# **1. Entender el entorno**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Se usará el taxi-v3 del entorno ginmasio de OpenAI.\n",
        "\n",
        "* El entorno OpenAI Gym Taxi-v3 es un problema clásico de aprendizaje por refuerzo que se utiliza a menudo para aprender y probar algoritmos RL. En este entorno, el agente controla un taxi que navega en un mundo en red, para recoger a un pasajero de un lugar y dejarlo en otro.\n",
        "* La matriz de funcionamiento consistirá en 5 * 5 celdas.\n",
        "* El taxista se muestra con un fondo amarillo.\n",
        "* Las paredes se representarán meidante líneas verticales.\n",
        "* El objetivo es mover el taxi a la ubicación del pasajero (de color azul), recoger al pasajero, trasladarse al destino deseado del pasajero (de color violeta) y dejar al pasajero.\n",
        "* El agente recibe las siguientes recompensas:\n",
        " * +20 por dejar con éxito al pasajero.\n",
        " * -10 por cada intento fallido de recoger o dejar al pasajero.\n",
        " * -1 por cada paso dado por el agente, con el objetivo de incentivarlo a tomar una ruta eficiente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwWbvdMzHG9D",
        "outputId": "19055e67-dd62-41c0-dcff-dfc73a690612"
      },
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# IMPORTAMOS LA LIBRERÍA gymnasium Y MOSTRAMOS EL ENTORNO\n",
        "#-------------------------------------------------------------------------\n",
        "import gymnasium as gym\n",
        "entorno = gym.make( 'Taxi-v3', render_mode = 'ansi' )\n",
        "entorno.reset()\n",
        "\n",
        "print( entorno.render() )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNJcpfTjHVmA"
      },
      "source": [
        "\n",
        "\n",
        "*   La línea gym.make('Taxi-v3′, render_mode='ansi') crea una instancia del entorno Taxi-v3. El argumento render_mode=’ansi’ especifica que el modo de representación será el modo ANSI, que es un modo basado en texto adecuado para mostrar el entorno en una consola de texto.\n",
        "*   Se llama al método entorno.reset() para restablecer el entorno a su estado inicial. Por lo general, esto se hace al comienzo de cada episodio para comenzar de nuevo.\n",
        "\n",
        "# **2. Crear el agente de q-aprendizaje**\n",
        "\n",
        "\n",
        "*   Initialización (método __init__):\n",
        " * entorno: Entorno en el que opera el agente.\n",
        " * tasaAprendizaje: Tasa de aprendizaje para actualizar los valores Q.\n",
        " * epsilonInicial: Tasa de exploración inicial.\n",
        " * epsilonDecremento: Velocidad en la que disminuye la tasa de exploración.\n",
        " * epsilonFinal: Tasa mínima de exploración.\n",
        " * factorDecremento: Factor de descuento para recompensas futuras.\n",
        "*   Método obtener_accion:\n",
        " * Con probabilidad ε, elige una acción aleatoria (exploración).\n",
        " * Con probabilidad 1-ε, elige la acción con el valor Q más alto para la observación actual (explotación).\n",
        " * Este valor es alto inicialmente y se reduce gradualmente.\n",
        "*   Método update:\n",
        " * Actualiza el valor Q según la recompensa observada y el valor Q máximo del siguiente estado.\n",
        "*   Método decay_epsilon:\n",
        " * disminuye la tasa de exploración (épsilon) hasta alcanzar su valor final."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IVmjPdqzHRB8"
      },
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# IMPORTAMOS MÁS LIBRERÍAS NECESARIAS Y CREAMOS LA CLASE QAprendizajeAgente\n",
        "#-------------------------------------------------------------------------\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "class QAprendizajeAgente:\n",
        "    #---------------------------------------------------------------------\n",
        "    # Inicializamos la clase QAprendizajeAgente\n",
        "    #---------------------------------------------------------------------\n",
        "    def __init__( self, entorno, tasaAprendizaje, epsilonInicial, epsilonDecremento, epsilonFinal, factorDecremento = 0.95 ):\n",
        "        self.env              = entorno\n",
        "        self.learning_rate    = tasaAprendizaje\n",
        "        self.discount_factor  = factorDecremento\n",
        "        self.epsilon          = epsilonInicial\n",
        "        self.epsilon_decay    = epsilonDecremento\n",
        "        self.final_epsilon    = epsilonFinal\n",
        "\n",
        "        # Inicializamos un diccionario vacío de valores de acción de estado\n",
        "        self.q_values = defaultdict( lambda: np.zeros( entorno.action_space.n ) )\n",
        "\n",
        "    #---------------------------------------------------------------------\n",
        "    # Obtenemos la acción a desarrollar a partir del estado (posición) en\n",
        "    # el que se halle el vehículo (Taxi)\n",
        "    #---------------------------------------------------------------------\n",
        "    def obtenerAccion( self, estado ) -> int:\n",
        "        x = np.random.rand()\n",
        "        if x < self.final_epsilon:\n",
        "            return self.env.action_space.sample()\n",
        "        else:\n",
        "            return np.argmax( self.q_values[ estado ] )\n",
        "\n",
        "    #---------------------------------------------------------------------\n",
        "    # Actualizamos los datos del nuevoQ\n",
        "    #---------------------------------------------------------------------\n",
        "    def actualizar( self, estado, accion, recompensa, fin, siguienteEstado ):\n",
        "        if not fin:\n",
        "            nuevoQ = np.max( self.q_values[ siguienteEstado ] )\n",
        "            self.q_values[ estado ][ accion ] += self.learning_rate * \\\n",
        "                ( recompensa + self.discount_factor * nuevoQ - self.q_values[ estado ][ accion ])\n",
        "\n",
        "    #---------------------------------------------------------------------\n",
        "    # Decrementamos epsilon en función de la tasa de esploración de épsilon\n",
        "    #---------------------------------------------------------------------\n",
        "    def decrementarEpsilon( self ):\n",
        "        self.epsilon = max( self.final_epsilon, self.epsilon - self.epsilon_decay )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIcEbgWvJe7u"
      },
      "source": [
        "# **3. Definición del método de entrenamiento**\n",
        "\n",
        "*   La función entrenarAgente es responsable de entrenar al agente Q-learning proporcionado en un entorno determinado durante un número específico de episodios.\n",
        "\n",
        "*   Cada episodio es una iteración hasta que el agente comete un error que conduce a la terminación o completa la entrega con éxito.\n",
        "\n",
        "*   La función itera a través del número especificado de episodios (episodios).\n",
        "\n",
        "*   Episodio de Inicialización:\n",
        " * Restablece el entorno e inicializa variables para seguir el progreso del episodio.\n",
        "* Dentro de cada episodio, el agente:\n",
        " * Selecciona acciones basadas en el método definido en nuestra clase de agente,\n",
        " * Interactúa con el entorno,\n",
        " * actualiza los valores Q y acumula recompensas hasta que termina el episodio.\n",
        "* Disminución de Epsilon:\n",
        " * Después de cada episodio, la tasa de exploración del agente (épsilon) se reduce utilizando el método decay_epsilon.\n",
        " * Inicialmente, el valor épsilon es alto, lo que lleva a una mayor exploración.\n",
        " * Esto se reduce a 0,1 en la mitad del número de episodios.\n",
        "* Seguimiento del rendimiento:\n",
        " * La recompensa total de cada episodio se almacena en la lista de recompensas.\n",
        " * Calculamos el promedio de los últimos 10 episodios y guardamos la mejor recompensa promedio obtenida al final de cada episodio.\n",
        "* Mostrar el progreso:\n",
        " * Mostramos el mejor progreso promedio cada 100 intervalos de evaluación,\n",
        " * También nos devuelven todas las recompensas que obtuvimos en cada episodio. Esto nos ayudará a trazar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rdSDbKLFHpq5"
      },
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# CREAMOS LA FUNCIÓN DE ENTRENAMIENTO DEL AGENTE\n",
        "#-------------------------------------------------------------------------\n",
        "def entrenarAgente( agente, entorno, episodios, intervalo = 100 ):\n",
        "    recompensas = []\n",
        "    mejorRecompensa = -np.inf\n",
        "\n",
        "    for i in range( episodios ):\n",
        "        estado, _ = entorno.reset()\n",
        "        fin = exploracionTruncada = False\n",
        "        longitud = totalRecompensa = 0\n",
        "\n",
        "        while ( fin == False ) and ( exploracionTruncada == False ):\n",
        "            accion = agente.obtenerAccion( estado )\n",
        "            siguienteEstado, recompensa, fin, exploracionTruncada, _ = entorno.step( accion )\n",
        "\n",
        "            agente.actualizar( estado, accion, recompensa, fin, siguienteEstado )\n",
        "            estado = siguienteEstado\n",
        "            longitud = longitud + 1\n",
        "            totalRecompensa += recompensa\n",
        "\n",
        "        agente.decrementarEpsilon()\n",
        "        recompensas.append( totalRecompensa )\n",
        "\n",
        "        if i >= intervalo:\n",
        "            mediaRecompensas = np.mean( recompensas[ i - intervalo: i ] )\n",
        "            mejorRecompensa = max( mediaRecompensas, mejorRecompensa )\n",
        "        if i % intervalo == 0 and i > 0:\n",
        "            print( f\"Episodio {i} -> mejor recompensa = {mejorRecompensa} \" )\n",
        "\n",
        "    return recompensas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBNY7NJgJN1e"
      },
      "source": [
        "# **4. Ejecución del método de entrenamiento**\n",
        "\n",
        "*   Configura parámetros para la capacitación, como la cantidad de episodios, la tasa de aprendizaje, el factor de descuento y las tasas de exploración.\n",
        "\n",
        "*   Crea el entorno Taxi-v3 desde OpenAI Gym.\n",
        "\n",
        "*   Inicializa un agente Q-learning (QAprendizajeAgente) con los parámetros especificados.\n",
        "\n",
        "*   Llama a la función entrenarAgente para entrenar al agente utilizando el entorno y los parámetros especificados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXx_sovEHs4s",
        "outputId": "1af879eb-b49c-4876-87cf-4224a83ffd23"
      },
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# DEFINIMOS LAS CARACTERÍSTICAS DEL MODELO DE AGENTE\n",
        "#   El número de episodios se genera aleatoriamente en múltiplos de 100\n",
        "#   La tasa de aprendizaje también se hace aleatoria\n",
        "#-------------------------------------------------------------------------\n",
        "import random\n",
        "\n",
        "episodios         = random.randint( 1, 500 ) * 10    # 20000\n",
        "tasaAprendizaje   = random.uniform( 0.01, 1.00 )      # 0.25\n",
        "epsilonInicial    = 1\n",
        "epsilonFinal      = 0\n",
        "epsilonDecremento   = ( ( epsilonFinal - epsilonInicial ) / ( episodios / 2 ) )\n",
        "\n",
        "entorno = gym.make( 'Taxi-v3', render_mode = 'ansi' )\n",
        "agente  = QAprendizajeAgente( entorno, tasaAprendizaje, epsilonInicial, epsilonDecremento, epsilonFinal )\n",
        "valores = entrenarAgente( agente, entorno, episodios )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfySg3C4Hw3g"
      },
      "source": [
        "# **5. Dibujando los logros**\n",
        "\n",
        "*   Podemos trazar todas las recompensas obtenidas contra el episodio.\n",
        "*   Vemos una disminución gradual en el valor de la recompensa fa desde un gran valor negativo hacia cero y, en última instancia, alcanza un valor positivo alrededor de 8,6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "l6ak5HZAHxYw",
        "outputId": "d97bcdd1-f87a-4eb7-cc64-5d0f5a4a1087"
      },
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# MOSTRAMOS EL NIVEL DE APRENDIZAJE DEL MODELO\n",
        "#-------------------------------------------------------------------------\n",
        "def dibujarValores( valores ):\n",
        "    plt.plot( np.arange( len( valores ) ), valores )\n",
        "    plt.title( 'Resultados de los Episodios' )\n",
        "    plt.xlabel( 'Episodio' )\n",
        "    plt.ylabel( 'Resultado' )\n",
        "    plt.show()\n",
        "\n",
        "dibujarValores( valores )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9elLuGRH_Oj"
      },
      "source": [
        "# **6.   Ejecutando el Agente**\n",
        "\n",
        "La función run_agent está diseñada para ejecutar nuestro agente capacitado en el entorno Taxi-v3 y muestra su interacción.\n",
        "\n",
        "*   agente.epsilon = 0: Esta línea establece la tasa de exploración (épsilon) del agente en cero, lo que indica que el agente debe explotar su política aprendida sin realizar más exploración.\n",
        "\n",
        "*   El bucle while continúa hasta que el episodio termina (terminado == Verdadero) o la interacción se trunca (truncada == Verdadero)\n",
        "\n",
        "*   accion = agente.obtenerAccion(estado): El agente selecciona una acción en función de su política aprendida.\n",
        "\n",
        "*   entorno.render(): Representa el estado actualizado después de la acción del agente.\n",
        "\n",
        "*   estado = siguienteEstado: Actualiza el estado actual al siguiente estado para la siguiente iteración.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbaU4wLHInNf",
        "outputId": "813fe1f9-81c5-4725-d209-87536c7ab8c7"
      },
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# MOSTRAMOS EL FUNCIONAMIENTO DEL AGENTE PARA LLEGAR AL OBJETIVO\n",
        "#-------------------------------------------------------------------------\n",
        "def ejecutarAgente( agente, entorno ):\n",
        "    agente.epsilon = 0          # No necesita seguir explorando\n",
        "    estado, _ = entorno.reset() # Obtiene el estado actual\n",
        "    entorno.render()\n",
        "    fin = exploracionTruncada = False\n",
        "\n",
        "    while fin == False and exploracionTruncada == False   :\n",
        "        accion = agente.obtenerAccion( estado )\n",
        "        siguienteEstado, _, fin, exploracionTruncada, _ = entorno.step( accion )\n",
        "        print( entorno.render() )\n",
        "        estado = siguienteEstado\n",
        "\n",
        "entorno = gym.make( 'Taxi-v3', render_mode = 'ansi' )\n",
        "ejecutarAgente( agente, entorno )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
