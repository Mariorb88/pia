{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzT6cQ1mOBRt",
        "outputId": "6a40a07b-9efc-4713-d643-4e8bf47d7ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Recolectamos datos del entorno de Taxi-v3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:28<00:00, 17.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2. Hacemos el preprocesamiento de los datos del entorno de Taxi-v3\n"
          ]
        }
      ],
      "source": [
        "#--------------------------------------------------------------------------\n",
        "#   MODELO DE AUTOAPRENDIZAJE CON TAXI\n",
        "#--------------------------------------------------------------------------\n",
        "#-- Este código recolecta datos del entorno Taxi-v3 y entrena un modelo para predecir el siguiente estado,\n",
        "#-- dado un estado actual y una acción. No se usan recompensas, por lo que es autoaprendizaje (self-supervised learning).\n",
        "#--------------------------------------------------------------------------\n",
        "\n",
        "#--------------------------------------------------------------------------\n",
        "# 1. Importamos las librerías necesarias\n",
        "#--------------------------------------------------------------------------\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#--------------------------------------------------------------------------\n",
        "# 2. Recolectamos datos del entorno Taxi-v3\n",
        "#--------------------------------------------------------------------------\n",
        "print( '1. Recolectamos datos del entorno de Taxi-v3' )\n",
        "entorno = gym.make( \"Taxi-v3\" )\n",
        "\n",
        "transitions = []\n",
        "EPISODIOS = 500  # Número de episodios para recolectar datos (500)\n",
        "\n",
        "for _ in tqdm(range(EPISODIOS)):\n",
        "    estado, _ = entorno.reset()\n",
        "    FIN = False\n",
        "    while not FIN:\n",
        "        action = entorno.action_space.sample()  # Elegimos una acción aleatoria (sin política)\n",
        "        proximo_estado, _, FIN, _, _ = entorno.step(action)\n",
        "        # Guardamos la transición (estado, acción, siguiente estado)\n",
        "        transitions.append((estado, action, proximo_estado))\n",
        "        estado = proximo_estado\n",
        "\n",
        "#--------------------------------------------------------------------------\n",
        "# 3. Preprocesamiento de datos\n",
        "#--------------------------------------------------------------------------\n",
        "print( '\\n2. Hacemos el preprocesamiento de los datos del entorno de Taxi-v3' )\n",
        "STATE_SIZE = entorno.observation_space.n\n",
        "ACTION_SIZE = entorno.action_space.n\n",
        "\n",
        "# One-hot encode states and actions\n",
        "def one_hot(index, size):\n",
        "    vec = np.zeros(size)\n",
        "    vec[index] = 1\n",
        "    return vec\n",
        "\n",
        "X = []  # Entradas: estado + acción (one-hot)\n",
        "y = []  # Salidas: siguiente estado (one-hot)\n",
        "\n",
        "for s, a, s_next in transitions:\n",
        "    state_vec = one_hot(s, STATE_SIZE)\n",
        "    action_vec = one_hot(a, ACTION_SIZE)\n",
        "    input_vec = np.concatenate([state_vec, action_vec])\n",
        "    target_vec = one_hot(s_next, STATE_SIZE)\n",
        "    X.append(input_vec)\n",
        "    y.append(target_vec)\n",
        "\n",
        "# Convertir a numpy.array antes de convertir en tensor\n",
        "X = np.array(X, dtype=np.float32)  # Convertir la lista a un solo array de numpy\n",
        "y = np.array(y, dtype=np.float32)  # Convertir la lista a un solo array de numpy\n",
        "\n",
        "# Ahora convierte a tensor de PyTorch\n",
        "X = torch.tensor(X)\n",
        "y = torch.tensor(y)\n",
        "\n",
        "#--------------------------------------------------------------------------\n",
        "# 4. Definimos el modelo como una Red neuronal para predecir el siguiente estado\n",
        "#--------------------------------------------------------------------------\n",
        "print( '\\n3. Definimos el modelo de red neuronal de datos del entorno de Taxi-v3' )\n",
        "class StatePredictor(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, output_size),\n",
        "            nn.Softmax(dim=1)  # Distribución sobre los estados posibles\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "MODELO = StatePredictor( STATE_SIZE + ACTION_SIZE, STATE_SIZE )\n",
        "criterion = nn.CrossEntropyLoss()  # Usamos clasificación categórica (una clase verdadera)\n",
        "optimizer = optim.Adam( MODELO.parameters(), lr = 0.01 ) # 0.001\n",
        "\n",
        "#--------------------------------------------------------------------------\n",
        "# 5. Realizamos el entrenamiento\n",
        "#--------------------------------------------------------------------------\n",
        "print( '\\n4. Realizamos el entrenamiento con el modelo de datos del entorno de Taxi-v3' )\n",
        "EPOCAS      = 10\n",
        "BATCH_SIZE  = 128  #64\n",
        "\n",
        "for epoch in range( EPOCAS ):\n",
        "    permutation = torch.randperm(X.size(0))\n",
        "    loss_total = 0\n",
        "    for i in range(0, X.size(0), BATCH_SIZE):\n",
        "        indices = permutation[i:i+BATCH_SIZE]\n",
        "        batch_x, batch_y = X[indices], y[indices]\n",
        "\n",
        "        preds = MODELO(batch_x)\n",
        "        loss = criterion(preds, torch.argmax(batch_y, dim=1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_total += loss.item()\n",
        "    print(f\"    Época {epoch+1}/{EPOCAS}, Pérdida: {loss_total:.4f}\")\n",
        "\n",
        "# Al finalizar, el modelo ha aprendido una función de transición aproximada:\n",
        "# (estado, acción) → siguiente estado probable\n",
        "\n",
        "#--------------------------------------------------------------------------\n",
        "# 6. Evaluamos el modelo con predicciones\n",
        "#--------------------------------------------------------------------------\n",
        "print( '\\n5. Mostramos la evaluación del modelo con prediccines de datos del entorno de Taxi-v3' )\n",
        "print( \"\\    Ejemplo de predicciones del modelo:\")\n",
        "\n",
        "estadoReal = []\n",
        "estadosPredichos = []\n",
        "accionesTomadas = []\n",
        "\n",
        "estado, _ = entorno.reset()\n",
        "for _ in range( 5 ):\n",
        "    accion = entorno.action_space.sample()\n",
        "    input_vec = np.concatenate([one_hot(estado, STATE_SIZE), one_hot(accion, ACTION_SIZE)])\n",
        "    input_tensor = torch.tensor([input_vec], dtype=torch.float32)\n",
        "\n",
        "    prediccion = MODELO(input_tensor)\n",
        "    estadoPredicho = torch.argmax(prediccion).item()\n",
        "\n",
        "    # Almacenar estados y predicciones\n",
        "    estadoReal.append(estado)\n",
        "    estadosPredichos.append(estadoPredicho)\n",
        "    accionesTomadas.append(accion)\n",
        "\n",
        "    print( f\"    Estado actual: {estado}, Acción: {accion} → Estado predicho: {estadoPredicho}\" )\n",
        "\n",
        "    # Visualizar el entorno\n",
        "    entorno.render()\n",
        "\n",
        "    # Realizar la acción en el entorno y obtener el siguiente estado\n",
        "    estado, _, FIN, _, _ = entorno.step(accion)\n",
        "    if FIN:\n",
        "        estado, _ = entorno.reset()\n",
        "\n",
        "#--------------------------------------------------------------------------\n",
        "# 7. Mostramos el gráfico de las transiciones\n",
        "#--------------------------------------------------------------------------\n",
        "print( '6. Mostramos el gráfico de las transicciones en el entorno de Taxi-v3' )\n",
        "plt.figure( figsize = ( 10, 6 ) )\n",
        "\n",
        "# Mostramos gráficamente el estado real vs estado predicho\n",
        "plt.subplot( 1, 2, 1 )\n",
        "plt.plot( estadoReal, label = 'Estado Real', marker = 'o' )\n",
        "plt.plot( estadosPredichos, label = 'Estado Predicho', marker = 'x' )\n",
        "plt.title( \"Estado Real vs Estado Predicho\")\n",
        "plt.xlabel( \"Paso\")\n",
        "plt.ylabel( \"Estado\")\n",
        "plt.legend()\n",
        "\n",
        "# Mostramos gráficamente las acciones tomadas\n",
        "plt.subplot( 1, 2, 2 )\n",
        "plt.plot( accionesTomadas, label = 'Acciones Tomadas', marker = 's', color = 'r' )\n",
        "plt.title( \"Acciones Tomadas\" )\n",
        "plt.xlabel( \"Paso\" )\n",
        "plt.ylabel( \"Acción\" )\n",
        "plt.legend()\n",
        "\n",
        "# Mostramos los gráficos\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Cerramos el entorno al final\n",
        "entorno.close()"
      ]
    }
  ]
}