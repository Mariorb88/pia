{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1X8wL82WJpS",
        "outputId": "067bde9d-db13-4a3e-9bcc-bd5bf8bbc379"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo  FashionMNIST.keras  con imágenes ya ajustadas.\n",
            "------------------------------------\n",
            "|  Lado de las imágenes     :  80  |\n",
            "------------------------------------\n",
            "-------------------------------------------------\n",
            "|  Objetivo de entrenamiento:  accuracy = 0.75  |\n",
            "-------------------------------------------------\n",
            "5250/5250 [==============================] - 681s 129ms/step - loss: 0.3765 - accuracy: 0.8644\n",
            "    Precisión en la vuelta 1:  86.44%\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 669\u001b[0m\n\u001b[0;32m    664\u001b[0m entrenarModelo( modelo, ACIERTOS, FICHERO, datosEntrenamiento, \u001b[38;5;241m1\u001b[39m, STEPS_PER_EPOCH, \u001b[38;5;28;01mTrue\u001b[39;00m )\n\u001b[0;32m    666\u001b[0m \u001b[38;5;66;03m#-------------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;66;03m# 7. EVALUACIÓN DEL MODELO\u001b[39;00m\n\u001b[0;32m    668\u001b[0m \u001b[38;5;66;03m#-------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m--> 669\u001b[0m \u001b[43mevaluarModelo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatosPrueba\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;66;03m#-------------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;66;03m# 8. Hacemos predicciones (modelo, datosPrueba, nombreClases )\u001b[39;00m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;66;03m#-------------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    674\u001b[0m realizarPedicciones( modelo, datosPrueba, nombreClases, \u001b[38;5;28;01mTrue\u001b[39;00m )\n",
            "Cell \u001b[1;32mIn[11], line 435\u001b[0m, in \u001b[0;36mevaluarModelo\u001b[1;34m(Modelo, Datos, Realizar)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;124;03mEvalúa un modelo en un conjunto de datos y muestra la pérdida y precisión.\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;124;03m:param Realizar: Si es True, realiza la evaluación; si es False, no hace nada.\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Realizar:\n\u001b[1;32m--> 435\u001b[0m     loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mModelo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mDatos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPérdida en el conjunto de prueba: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m )\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecisión en el conjunto de prueba: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n",
            "File \u001b[1;32mc:\\Users\\Mario\\FP\\tf_cpu_env\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\Mario\\FP\\tf_cpu_env\\lib\\site-packages\\keras\\src\\engine\\training.py:2200\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   2196\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   2197\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2198\u001b[0m             ):\n\u001b[0;32m   2199\u001b[0m                 callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 2200\u001b[0m                 logs \u001b[38;5;241m=\u001b[39m \u001b[43mtest_function_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2201\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2202\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdata_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2203\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2204\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pss_evaluation_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2205\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2207\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2208\u001b[0m \u001b[38;5;66;03m# Override with model metrics instead of last step logs\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Mario\\FP\\tf_cpu_env\\lib\\site-packages\\keras\\src\\engine\\training.py:4000\u001b[0m, in \u001b[0;36m_TestFunction.run_step\u001b[1;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[0;32m   3999\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_or_iterator, data_handler, step, unused_shards):\n\u001b[1;32m-> 4000\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4001\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   4002\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
            "File \u001b[1;32mc:\\Users\\Mario\\FP\\tf_cpu_env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\Mario\\FP\\tf_cpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Users\\Mario\\FP\\tf_cpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:864\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    862\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 864\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Mario\\FP\\tf_cpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Mario\\FP\\tf_cpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\Users\\Mario\\FP\\tf_cpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
            "File \u001b[1;32mc:\\Users\\Mario\\FP\\tf_cpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
            "File \u001b[1;32mc:\\Users\\Mario\\FP\\tf_cpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# 0. Constantes\n",
        "#-------------------------------------------------------------------------------\n",
        "SPLITS  = { 'test', 'train', 'validation' }\n",
        "\n",
        "# Configuración inicial\n",
        "LADO          = 80\n",
        "TAMAÑO_IMAGEN = [LADO, LADO]\n",
        "TAMAÑO_LOTE   = 8\n",
        "\n",
        "# Definimos el tamaño de los splits\n",
        "TRAIN = 0.60\n",
        "TEST  = round( ( ( 1 - TRAIN ) / 1 ), 2 )\n",
        "\n",
        "# Variables asociadas al modelo\n",
        "TAMAÑO_KERNEL = ( 3, 3 )\n",
        "TAMAÑO_POOL   = ( 2, 2 )\n",
        "ACTIVACION_TIPO   = 'relu'\n",
        "ACTIVACION_SALIDA = 'softmax'\n",
        "INPUT_SHAPE       = ( LADO, LADO, 1 )\n",
        "FILTROS = 32\n",
        "DROPOUT = 0.25\n",
        "\n",
        "# Variable asociada al entrenamiento\n",
        "ACIERTOS = 0.7500\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# FUNCIONES\n",
        "#-------------------------------------------------------------------------------\n",
        "def mostrarTexto( texto ):\n",
        "    \"\"\"\n",
        "    Muestra un texto enmarcado con guiones por encima y por debajo.\n",
        "\n",
        "    :param texto: El texto a mostrar en formato enmarcado.\n",
        "    \"\"\"\n",
        "    longitud = len( texto )\n",
        "    print( '-' * longitud )\n",
        "    print( texto )\n",
        "    print( '-' * longitud )\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# 1. Importamos las librerías\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# 2. Descargamos el dataset\n",
        "#-------------------------------------------------------------------------------\n",
        "def obtenerInformacionDataset( Dataset ):\n",
        "    \"\"\"\n",
        "    Obtiene la información del dataset de TensorFlow Datasets.\n",
        "\n",
        "    :param Dataset: Nombre del dataset (ej. 'cifar10').\n",
        "    :return:        Información del dataset.\n",
        "    \"\"\"\n",
        "    info = tfds.builder( Dataset ).info\n",
        "    return info\n",
        "#-------------------------------------------------------------------------------\n",
        "def obtenerNombreDataset( Splits ):\n",
        "    \"\"\"\n",
        "    Solicita al usuario un nombre de dataset de TensorFlow Datasets y verifica que el dataset\n",
        "    tenga características de 'image' y 'label', y que incluya alguno de los splits requeridos.\n",
        "\n",
        "    :param Splits: Conjunto de splits necesarios (ej. {'test', 'train', 'validation'}).\n",
        "    :return: Una tupla con el nombre del dataset y su información.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        try:\n",
        "            dataset = input( \"Nombre del dataset (por ejemplo, 'cifar10'): \")\n",
        "            info    = obtenerInformacionDataset( dataset )\n",
        "            if \"image\" in info.features and \"label\" in info.features:\n",
        "                splits = set( info.splits.keys() )\n",
        "                if splits & Splits:\n",
        "                    break  # Si no hay errores, salimos del bucle\n",
        "\n",
        "        except Exception as e:\n",
        "            print( \"Error.\" )\n",
        "\n",
        "    return dataset, info\n",
        "#-------------------------------------------------------------------------------\n",
        "def obtenerNumeroClases( Dataset ):\n",
        "    \"\"\"\n",
        "    Obtiene el número de clases de un dataset de TensorFlow Datasets.\n",
        "\n",
        "    :param Dataset: Nombre del dataset (ej. 'cifar10').\n",
        "    :return: Número de clases del dataset.\n",
        "    \"\"\"\n",
        "    info = tfds.builder( Dataset ).info\n",
        "    return info.features[ 'label' ].num_classes\n",
        "#-------------------------------------------------------------------------------\n",
        "def obtenerNombreClases( Dataset ):\n",
        "    \"\"\"\n",
        "    Obtiene los nombres de las clases de un dataset de TensorFlow Datasets.\n",
        "\n",
        "    :param Dataset: Nombre del dataset (ej. 'cifar10').\n",
        "    :return: Lista de nombres de las clases del dataset o None si ocurre un error.\n",
        "    \"\"\"\n",
        "    info = obtenerInformacionDataset( Dataset )\n",
        "\n",
        "    return info.features[ 'label' ].names\n",
        "#-------------------------------------------------------------------------------\n",
        "def cargarDataset( Dataset, Splits ):\n",
        "    \"\"\"\n",
        "    Carga un dataset de TensorFlow Datasets con los splits especificados.\n",
        "\n",
        "    :param Dataset: Nombre del dataset (ej. 'cifar10').\n",
        "    :param Splits:  Lista de splits a cargar (ej. ['test', 'train', 'validation']).\n",
        "    :return:        El dataset cargado.\n",
        "    \"\"\"\n",
        "    info = obtenerInformacionDataset( Dataset )\n",
        "\n",
        "    # Establecer as_supervised en True salvo que el dataset no soporte el modo supervisado\n",
        "    as_supervised = True if info.supervised_keys else False\n",
        "\n",
        "    datasetCargado = tfds.load( Dataset,\n",
        "                                split='+'.join([split for split in Splits if split in info.splits]),\n",
        "                                as_supervised = as_supervised,\n",
        "                                download=True)\n",
        "\n",
        "    return datasetCargado\n",
        "#-------------------------------------------------------------------------------\n",
        "def contarTuplasDataset( Dataset, Splits ):\n",
        "    \"\"\"\n",
        "    Cuenta el número total de ejemplos en los splits especificados de un dataset de TensorFlow Datasets.\n",
        "\n",
        "    :param Dataset: Nombre del dataset (ej. 'cifar10').\n",
        "    :param Splits:  Lista de splits a contar (ej. ['test', 'train', 'validation']).\n",
        "    :return:        Número total de ejemplos en los splits especificados.\n",
        "    \"\"\"\n",
        "    info      = obtenerInformacionDataset( Dataset )\n",
        "    numTuplas = sum( [ info.splits[ split ].num_examples for split in Splits if split in info.splits ] )\n",
        "\n",
        "    return numTuplas\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# 3. Agrupamos los datos en Entrenamiento (60%) y Prueba (40%)\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# 4. Normalizamos las imágenes a blanco y negro, a un tamaño y las metemos en un buffer\n",
        "#-------------------------------------------------------------------------------\n",
        "# Función para normalizar imágenes\n",
        "def NORMALIZAR( Imagenes, Etiquetas, TamañoImagen ):\n",
        "    \"\"\"\n",
        "    Normaliza las imágenes y etiquetas para prepararlas para el entrenamiento.\n",
        "\n",
        "    :param Imagenes: Tensor de imágenes.\n",
        "    :param Etiquetas: Tensor de etiquetas correspondientes a las imágenes.\n",
        "    :param TamañoImagen: Tupla (altura, anchura) para redimensionar las imágenes.\n",
        "    :return: Tupla con las imágenes normalizadas y las etiquetas.\n",
        "    \"\"\"\n",
        "    Imagenes = tf.image.resize( Imagenes, TamañoImagen )  # Redimensionar imágenes\n",
        "    Imagenes = tf.cast( Imagenes, tf.float32 )            # Convertir a float32\n",
        "    Imagenes = Imagenes / 255.0                           # Normalizar a rango [0, 1]\n",
        "    if Imagenes.shape[-1] == 3:                           # Verificar si la imagen es RGB\n",
        "        Imagenes = tf.image.rgb_to_grayscale( Imagenes )  # Convertir a escala de grises\n",
        "\n",
        "    return Imagenes, Etiquetas\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# 5. Cambiamos diversos aspectos de las imágenes\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "#-------------------------------------------------------------------------------\n",
        "# 6. DEFINIMOS EL MODELO\n",
        "#-------------------------------------------------------------------------------\n",
        "# - Clasificación de capas del modelo\n",
        "#\n",
        "#   1.  Partes del modelo:\n",
        "#       - Capa de entrada de datos:\n",
        "#              Input( shape = ( altura, anchura, canales ) )\n",
        "#              Conv2D( filters = 64, kernel_size = ( 3, 3 ), input_shape = ( 32, 32, 3 ) )\n",
        "#                      filters         Número de filtros convolucionales\n",
        "#                      kernel_size     Tamaño de la ventana de convulación. Si es un único valor, se duplica\n",
        "#\n",
        "#       - Capa de procesamiento de datos:\n",
        "#              Conv2D( filters = 64, kernel_size = ( 3, 3 ), strides = ( 1, 1 ), padding = '', activation = 'relu' )\n",
        "#                      strides      Desplazamiento al hacer convulaciones\n",
        "#\n",
        "#                      padding      La convulación reduce el tamaño de la imagen, pero el padding agrega 0s a los bordes\n",
        "#                           valid       sin relleno\n",
        "#                           same        relleno uniforme. Si, además, padding = 1, entonces la salida tiene el mismo tamaño que la entrada\n",
        "#\n",
        "#                      activation    Función de activación que se aplica tras la convulación\n",
        "#                           ReLU        función lineal y no saturada\n",
        "#                           Elu         similar a ReLU pero con comportamiento suave para valores negativos\n",
        "#\n",
        "#              BatchNormalization()\n",
        "#\n",
        "#              MaxPooling2D( pool_size = ( 2, 2 ), strides = ( 1, 1 ), padding = 'valid/same' )\n",
        "#                       pool_size    Escala de reducción\n",
        "#                       strides      Desplazamiento\n",
        "#                       padding      Relleno o no\n",
        "#\n",
        "#              Dropout( rate = 0.25 )\n",
        "#                       rate          Fracción de unidades de entrada que se borrarán\n",
        "#\n",
        "#       - Capa de salida de datos:\n",
        "#               Dense( units, activation = 'softmax' )\n",
        "#                       units         Número de valores de salida\n",
        "#                       activation    Función de activación para devolver datos de salida\n",
        "#                           softmax     función de salida que escala los valores en probabilidades\n",
        "#                           sigmoid     función de salida para problemas de clasificación binaria\n",
        "#                           TanH        similar a sigmoid pero con un rango de salida de [ -1, 1 ]\n",
        "#\n",
        "#\n",
        "#   2.  Tipos de conexión:            Densas(Dense), Convolucionales (Conv1D, Conv2D. Conv3D, SeparableConv2D )\n",
        "#       Dense     Cada filtro está conectado con todos los filtros de cada capa\n",
        "#       Conv2D    Cada filtro NO se conecta con todos los filtros de cada capa\n",
        "#\n",
        "#   2.  Extracción de la información más representativa:  MaxPooling obtiene el valor máximo en la región pool_size\n",
        "#       MaxPooling2D( pool_size = ( 2, 2 ) )\n",
        "#                                   12  5 |  3  9 |  1 23\n",
        "#         12  5  3  9  1 23          8  6 | 55  6 | 13 62\n",
        "#          8  6 55  6 13 62         ---------------------\n",
        "#         41  8  3 51 22 27         41  8 |  3 51 | 22 27       12  55  23\n",
        "#         32 67 53 12 26 17   ===>  32 67 | 53 12 | 26 17   ==> 67  53  27\n",
        "#          9 22 15  8 57 62         ---------------------       53  75  62\n",
        "#         23 53 25 75 12  9          9 22 | 15  8 | 57 62\n",
        "#                                   23 53 | 25 75 | 12  9\n",
        "#\n",
        "#   3.  Aplanamiento de los datos:    Flatten\n",
        "#         1 1 0\n",
        "#         4 2 1     Flattening    1 1 0 4 2 1 0 2 1\n",
        "#         0 2 1\n",
        "#\n",
        "#   4.  Eliminación de conexiones entre neuronas según una probabilidad\n",
        "#       Dropout( 0.25 )\n",
        "#\n",
        "#   5.  Normalización y estabilización de las capas previas, acelerando el entrenamiento\n",
        "#       BatchNormalization()\n",
        "#\n",
        "#-------------------------------------------------------------------------------\n",
        "#-------------------------------------------------------------------------------\n",
        "def crearModelo( NumeroClases, InputShape, Filtros, TamañoKernel, TamañoPool, Activacion, ActivacionSalida, Borrar ):\n",
        "    \"\"\"\n",
        "    Crea un modelo de red neuronal convolucional para clasificación multiclase utilizando Keras.\n",
        "\n",
        "    :param NumeroClases:      Número de clases de salida (dimensión de la capa de salida).\n",
        "    :param InputShape:        Dimensión de la entrada (ej. (64, 64, 3) para imágenes de 64x64 RGB).\n",
        "    :param Filtros:           Número de filtros en la primera capa convolucional.\n",
        "    :param TamañoKernel:      Tamaño del kernel en las capas convolucionales.\n",
        "    :param TamañoPool:        Tamaño del pooling en las capas de MaxPooling.\n",
        "    :param Activacion:        Función de activación para las capas convolucionales y densas.\n",
        "    :param ActivacionSalida:  Función de activación para la capa de salida (ej. 'softmax' para multiclase).\n",
        "    :param Borrar:            Tasa de Dropout para las capas de regularización.\n",
        "    :return:                  Modelo compilado de Keras listo para entrenamiento.\n",
        "    \"\"\"\n",
        "    Modelo = Sequential()\n",
        "\n",
        "    Modelo.add( Input( shape = InputShape ) )\n",
        "    # Capa convolucional 1\n",
        "    Modelo.add( Conv2D( Filtros, kernel_size = TamañoKernel, activation = Activacion ) )\n",
        "    #Modelo.add( Conv2D( FILTROS, kernel_size = TAMAÑO_KERNEL, activation = ACTIVACION_TIPO, input_shape = INPUT_SHAPE ) )\n",
        "    Modelo.add( BatchNormalization() )  # Normalización por lotes\n",
        "    Modelo.add( MaxPooling2D( pool_size = TamañoPool ) )\n",
        "    Modelo.add( Dropout( Borrar ) )\n",
        "    \n",
        "\n",
        "    # Capa convolucional 2\n",
        "    Modelo.add( Conv2D( Filtros * 2, kernel_size = TamañoKernel, activation = Activacion ) )\n",
        "    Modelo.add( BatchNormalization() )  # Normalización por lotes\n",
        "    Modelo.add( MaxPooling2D( pool_size = TamañoPool ) )\n",
        "    Modelo.add( Dropout( Borrar ) )\n",
        "\n",
        "    # Capa convolucional 3\n",
        "    Modelo.add( Conv2D( Filtros * 3, kernel_size = TamañoKernel, activation = Activacion ) )\n",
        "    Modelo.add( BatchNormalization() )  # Normalización por lotes\n",
        "    Modelo.add( MaxPooling2D( pool_size = TamañoPool ) )\n",
        "    Modelo.add( Dropout( Borrar ) )\n",
        "\n",
        "    # Capa convolucional 4\n",
        "    Modelo.add( Conv2D( Filtros * 4, kernel_size = TamañoKernel, activation = Activacion ) )\n",
        "    Modelo.add( BatchNormalization() )  # Normalización por lotes\n",
        "    Modelo.add( MaxPooling2D( pool_size = TamañoPool ) )\n",
        "    Modelo.add( Dropout( Borrar ) )\n",
        "\n",
        "    # Aplanar la salida\n",
        "    Modelo.add( Flatten() )\n",
        "\n",
        "    # Capa densa para la clasificación\n",
        "    Modelo.add( Dense( Filtros * 5, activation = Activacion ) )\n",
        "    Modelo.add( Dropout( Borrar ) )\n",
        "\n",
        "    # Capa densa completamente conectada\n",
        "    Modelo.add( Dense( NumeroClases, activation = ActivacionSalida ) )  # Para clasificación multiclase\n",
        "\n",
        "    return Modelo\n",
        "\n",
        "def crearModelo_v1(NumeroClases, InputShape, Filtros, TamañoKernel, TamañoPool, Activacion, ActivacionSalida, Borrar):\n",
        "    Modelo = Sequential()\n",
        "    Modelo.add(Input(shape=InputShape))\n",
        "\n",
        "    # Capa convolucional 1\n",
        "    Modelo.add(Conv2D(Filtros, kernel_size=TamañoKernel, activation=Activacion))\n",
        "    Modelo.add(BatchNormalization())\n",
        "    Modelo.add(MaxPooling2D(pool_size=TamañoPool))\n",
        "    Modelo.add(Dropout(Borrar))\n",
        "\n",
        "    # Capa convolucional 2\n",
        "    Modelo.add(Conv2D(Filtros * 2, kernel_size=TamañoKernel, activation=Activacion))\n",
        "    Modelo.add(BatchNormalization())\n",
        "    Modelo.add(MaxPooling2D(pool_size=TamañoPool))\n",
        "    Modelo.add(Dropout(Borrar))\n",
        "\n",
        "    # Aplanar la salida\n",
        "    Modelo.add(Flatten())\n",
        "\n",
        "    # Capa densa\n",
        "    Modelo.add(Dense(Filtros * 2, activation=Activacion))\n",
        "    Modelo.add(Dropout(Borrar))\n",
        "\n",
        "    # Capa de salida\n",
        "    Modelo.add(Dense(NumeroClases, activation=ActivacionSalida))\n",
        "    return Modelo\n",
        "\n",
        "def crearModelo_v2(NumeroClases, InputShape, Filtros, TamañoKernel, TamañoPool, ActivacionSalida, Borrar):\n",
        "    Modelo = Sequential()\n",
        "    Modelo.add(Input(shape=InputShape))\n",
        "\n",
        "    # Capa convolucional 1\n",
        "    Modelo.add(Conv2D(Filtros // 2, kernel_size=TamañoKernel, activation='tanh'))\n",
        "    Modelo.add(BatchNormalization())\n",
        "    Modelo.add(MaxPooling2D(pool_size=TamañoPool))\n",
        "    Modelo.add(Dropout(Borrar))\n",
        "\n",
        "    # Capa convolucional 2\n",
        "    Modelo.add(Conv2D(Filtros, kernel_size=TamañoKernel, activation='tanh'))\n",
        "    Modelo.add(BatchNormalization())\n",
        "    Modelo.add(MaxPooling2D(pool_size=TamañoPool))\n",
        "    Modelo.add(Dropout(Borrar))\n",
        "\n",
        "    # Capa convolucional 3\n",
        "    Modelo.add(Conv2D(Filtros * 1.5, kernel_size=TamañoKernel, activation='tanh'))\n",
        "    Modelo.add(BatchNormalization())\n",
        "    Modelo.add(MaxPooling2D(pool_size=TamañoPool))\n",
        "    Modelo.add(Dropout(Borrar))\n",
        "\n",
        "    # Capa convolucional 4\n",
        "    Modelo.add(Conv2D(Filtros * 2, kernel_size=TamañoKernel, activation='tanh'))\n",
        "    Modelo.add(BatchNormalization())\n",
        "    Modelo.add(MaxPooling2D(pool_size=TamañoPool))\n",
        "    Modelo.add(Dropout(Borrar))\n",
        "\n",
        "    # Aplanamiento y capas densas\n",
        "    Modelo.add(Flatten())\n",
        "    Modelo.add(Dense(Filtros * 2, activation='tanh'))\n",
        "    Modelo.add(Dropout(Borrar))\n",
        "    Modelo.add(Dense(NumeroClases, activation=ActivacionSalida))\n",
        "    return Modelo\n",
        "\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "#-------------------------------------------------------------------------------\n",
        "# 7. COMPILAMOS EL MODELO\n",
        "#-------------------------------------------------------------------------------\n",
        "#\n",
        "#   1. Nombre del optimizador:    optimizer = adam\n",
        "#\n",
        "#   2. Función de pérdida:        loss = 'sparse_categorical_crossentropy'\n",
        "#\n",
        "#   3. Métricas que se evalúan:   [ 'accuracy' ]\n",
        "#\n",
        "#-------------------------------------------------------------------------------\n",
        "#-------------------------------------------------------------------------------\n",
        "def compilarModelo( Modelo, Optimizador, Perdida, Metrica, Realizar ):\n",
        "    \"\"\"\n",
        "    Compila el modelo con el optimizador, la función de pérdida y la métrica proporcionados.\n",
        "\n",
        "    :param Modelo:      El modelo de Keras a compilar.\n",
        "    :param Optimizador: Nombre o instancia del optimizador (ej. 'adam').\n",
        "    :param Perdida:     Nombre de la función de pérdida (ej. 'sparse_categorical_crossentropy').\n",
        "    :param Metrica:     Nombre de la métrica a usar (ej. 'accuracy').\n",
        "    :param Realizar:    Booleano que indica si se debe proceder con la compilación.\n",
        "    :return:            True si el modelo fue compilado, False en caso contrario.\n",
        "    \"\"\"\n",
        "    if Realizar:\n",
        "        Modelo.compile( optimizer = Optimizador, loss = Perdida, metrics = [ Metrica ] )\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# 8. Entrenamos el modelo\n",
        "#-------------------------------------------------------------------------------\n",
        "# .keras    arquitectura + parámetros\n",
        "#\n",
        "# .h5       sólo los parámetros del modelo\n",
        "#      modelo_cargado.load_weights()\n",
        "#\n",
        "# directorio  arquitectura + parámetros + metadatos\n",
        "#-------------------------------------------------------------------------------\n",
        "def entrenarModelo( Modelo, Aciertos, Fichero, Datos, Epocas, Lotes, Realizar ):\n",
        "    \"\"\"\n",
        "    Entrena un modelo hasta que la precisión alcance el umbral deseado o hasta que se detenga el proceso.\n",
        "\n",
        "    :param Modelo: El modelo de Keras a entrenar.\n",
        "    :param Aciertos: Precisión objetivo para detener el entrenamiento (entre 0 y 1).\n",
        "    :param Fichero: Ruta del archivo donde se guardará el modelo.\n",
        "    :param Datos: Conjunto de datos para entrenar el modelo.\n",
        "    :param Epocas: Número de épocas para entrenar el modelo en cada iteración.\n",
        "    :param Lotes: Número de pasos por época (batches).\n",
        "    :param Realizar: Si es True, realiza el entrenamiento; si es False, no hace nada.\n",
        "    \"\"\"\n",
        "    if Realizar:\n",
        "        EJECUCIONES = 0\n",
        "        while True:\n",
        "            historial = Modelo.fit( Datos, epochs = Epocas, steps_per_epoch = Lotes )\n",
        "            Modelo.save( Fichero )\n",
        "            precision_actual = historial.history[ 'accuracy' ][ 0 ]\n",
        "            EJECUCIONES = EJECUCIONES + 1\n",
        "            print( f\"    Precisión en la vuelta {EJECUCIONES}:  {precision_actual * 100:.2f}%\" )\n",
        "            if precision_actual >= Aciertos:\n",
        "                break\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# 9. Evaluamos el modelo\n",
        "#-------------------------------------------------------------------------------\n",
        "def evaluarModelo( Modelo, Datos, Realizar ):\n",
        "    \"\"\"\n",
        "    Evalúa un modelo en un conjunto de datos y muestra la pérdida y precisión.\n",
        "\n",
        "    :param Modelo: El modelo de Keras a evaluar.\n",
        "    :param Datos:  Conjunto de datos en el que se evaluará el modelo.\n",
        "    :param Realizar: Si es True, realiza la evaluación; si es False, no hace nada.\n",
        "    \"\"\"\n",
        "    if Realizar:\n",
        "        loss, accuracy = Modelo.evaluate( Datos, verbose = 0 )\n",
        "        print( f\"Pérdida en el conjunto de prueba: {loss:.2f}\" )\n",
        "        print( f\"Precisión en el conjunto de prueba: {accuracy * 100:.2f}%\" )\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# 10. Hacemos predicciones\n",
        "#-------------------------------------------------------------------------------\n",
        "def obtenerLotesImagenesEtiquetas( Datos ):\n",
        "    \"\"\"\n",
        "    Obtiene un lote de imágenes y etiquetas del conjunto de datos.\n",
        "\n",
        "    :param Datos: Dataset de TensorFlow (tf.data.Dataset) del que se extraerá el lote.\n",
        "    :return: Tupla (loteImagenes, loteEtiquetas) donde:\n",
        "             - loteImagenes: Tensor con un lote de imágenes.\n",
        "             - loteEtiquetas: Tensor con las etiquetas correspondientes a las imágenes.\n",
        "    \"\"\"\n",
        "    loteImagenes, loteEtiquetas = next( iter( Datos ) )\n",
        "\n",
        "    return loteImagenes, loteEtiquetas\n",
        "#-------------------------------------------------------------------------------\n",
        "def obtenerLoteNP( Lote ):\n",
        "    \"\"\"\n",
        "    Convierte un lote de datos de un tensor de TensorFlow a un arreglo NumPy.\n",
        "\n",
        "    :param Lote: Tensor de TensorFlow que se desea convertir a NumPy.\n",
        "    :return: Arreglo NumPy correspondiente al tensor.\n",
        "    \"\"\"\n",
        "    return Lote.numpy()\n",
        "#-------------------------------------------------------------------------------\n",
        "def obtenerPredicciones( Modelo, Lote ):\n",
        "    \"\"\"\n",
        "    Obtiene las predicciones del modelo sobre un lote de datos y calcula las etiquetas predichas\n",
        "    y los porcentajes de predicción.\n",
        "\n",
        "    :param Modelo: El modelo de Keras a usar para hacer las predicciones.\n",
        "    :param Lote: Tensor o arreglo de datos de entrada para el modelo.\n",
        "    :return: Tupla (etiquetasPredichas, porcentajesPredicciones) donde:\n",
        "             - etiquetasPredichas: Arreglo con las etiquetas predichas para cada muestra en el lote.\n",
        "             - porcentajesPredicciones: Arreglo con los porcentajes de la predicción más alta para cada muestra.\n",
        "    \"\"\"\n",
        "    predicciones            = Modelo.predict( Lote )\n",
        "    etiquetasPredichas      = np.argmax( predicciones, axis = -1 )\n",
        "    porcentajesPredicciones = np.max( predicciones, axis = -1 ) * 100\n",
        "\n",
        "    return etiquetasPredichas, porcentajesPredicciones\n",
        "#-------------------------------------------------------------------------------\n",
        "def seleccionarIndicesAleatorios( NumEtiquetas, Tamaño ):\n",
        "    \"\"\"\n",
        "    Selecciona índices aleatorios dentro del rango de etiquetas disponibles.\n",
        "\n",
        "    :param NumEtiquetas: Número total de etiquetas (rango de índices disponibles).\n",
        "    :param Tamaño: Número de índices a seleccionar.\n",
        "    :return: Arreglo de índices aleatorios seleccionados.\n",
        "    \"\"\"\n",
        "    indices = np.random.choice( NumEtiquetas, size = Tamaño )\n",
        "\n",
        "    return indices\n",
        "#-------------------------------------------------------------------------------\n",
        "def mostrarImagenes( Imagenes, EtiquetasReales, EtiquetasPredichas, PorcentajesPredicciones, Clases, NumImagenes = 10 ):\n",
        "    \"\"\"\n",
        "    Muestra un conjunto de imágenes con sus etiquetas reales, etiquetas predichas y porcentajes de predicción.\n",
        "\n",
        "    :param Imagenes:                Arreglo de imágenes a mostrar. Las imágenes deben estar en el rango [0, 1].\n",
        "    :param EtiquetasReales:         Arreglo de etiquetas reales correspondientes a las imágenes.\n",
        "    :param EtiquetasPredichas:      Arreglo de etiquetas predichas por el modelo.\n",
        "    :param PorcentajesPredicciones: Arreglo de porcentajes de la predicción más alta.\n",
        "    :param Clases:                  Lista de nombres de clases para las etiquetas.\n",
        "    :param NumImagenes:             Número de imágenes a mostrar.\n",
        "    \"\"\"\n",
        "    fig, axs = plt.subplots( 2, 5, figsize = ( 14, 7 ) )\n",
        "    #fig, axs = plt.subplots( 2, 5, figsize = ( 10, 7 ) )\n",
        "    Imagenes = Imagenes * 255.0  # Convertir imágenes de [0, 1] a [0, 255]\n",
        "    Imagenes = np.squeeze( Imagenes, axis = -1 )  # Convertir (H, W, 1) a (H, W)\n",
        "    indicesAleatorios = seleccionarIndicesAleatorios( len( EtiquetasReales ), 10 )\n",
        "\n",
        "\n",
        "    for i, idx in enumerate( indicesAleatorios ):\n",
        "        if i >= NumImagenes:\n",
        "            break\n",
        "\n",
        "        ax = axs[ i // 5, i % 5 ]\n",
        "        ax.imshow( Imagenes[ idx ], cmap = 'gray' )\n",
        "        ax.axis( 'off' )  # Ocultar los ejes\n",
        "        etiquetaReal, etiquetaPredicha, porcentajePrediccion = EtiquetasReales[ idx ], EtiquetasPredichas[ idx ], PorcentajesPredicciones[ idx ]\n",
        "\n",
        "        # Determinar el color del texto\n",
        "        color = 'blue' if etiquetaReal == etiquetaPredicha else 'red'\n",
        "\n",
        "        # Agregar título y texto debajo de la imagen\n",
        "        ax.set_title( f\"{Clases[ int( etiquetaReal ) ] }\", fontsize = 12, color = 'blue' )\n",
        "\n",
        "        # Texto debajo de la imagen\n",
        "        ax.text( 0.5, -0.08, f\"{Clases[ int( etiquetaPredicha ) ] } ({porcentajePrediccion:.2f}%)\",\n",
        "                 color = color, fontsize = 12, ha = 'center', va = 'center', transform = ax.transAxes )  # Usar coordenadas de ejes normalizados\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "#-------------------------------------------------------------------------------\n",
        "def realizarPedicciones( Modelo, Datos, nombreClases, Realizar ):\n",
        "    \"\"\"\n",
        "    Realiza predicciones utilizando el modelo dado sobre un lote de datos y muestra las imágenes con sus etiquetas reales,\n",
        "    etiquetas predichas y porcentajes de predicción.\n",
        "\n",
        "    :param Modelo: El modelo de Keras utilizado para hacer las predicciones.\n",
        "    :param Datos: Lote de datos (imágenes y etiquetas) sobre el que se harán las predicciones.\n",
        "    :param nombreClases: Lista de nombres de clases correspondientes a las etiquetas.\n",
        "    :param Realizar: Booleano que indica si se deben realizar las predicciones y mostrar los resultados.\n",
        "    \"\"\"\n",
        "    if Realizar:\n",
        "        loteImagenes, loteEtiquetas     = obtenerLotesImagenesEtiquetas( Datos )\n",
        "        loteImagenesNP, loteEtiquetasNP = obtenerLoteNP( loteImagenes ), obtenerLoteNP( loteEtiquetas )\n",
        "\n",
        "        # Hacer predicciones y calcular los porcentajes\n",
        "        etiquetasPredichas, porcentajesPredicciones = obtenerPredicciones( Modelo, loteImagenes )\n",
        "\n",
        "        # Usar nombreClases en lugar de recalcular los nombres de las clases\n",
        "        mostrarImagenes( loteImagenesNP, loteEtiquetasNP, etiquetasPredichas, porcentajesPredicciones, nombreClases )\n",
        "#-------------------------------------------------------------------------------\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# 1. Importamos las librerías\n",
        "#-------------------------------------------------------------------------------\n",
        "import tensorflow           as tf\n",
        "import tensorflow_datasets  as tfds\n",
        "import matplotlib.pyplot    as plt\n",
        "import numpy                as np\n",
        "import h5py\n",
        "import warnings\n",
        "import absl.logging\n",
        "import math\n",
        "import os\n",
        "\n",
        "from tensorflow.keras                     import Sequential\n",
        "from tensorflow.keras.layers              import BatchNormalization, Conv2D, Dense, Dropout, Flatten, Input, MaxPooling2D, Reshape\n",
        "from tensorflow.keras.models              import load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# 2.  CARGAMOS EL DATASET (fashion_mnist)\n",
        "#-------------------------------------------------------------------------------\n",
        "nombreDataset = input( \"Dataset que deseas cargar: \" )\n",
        "\n",
        "# Verificar si existe el archivo \".keras\"\n",
        "FICHERO = nombreDataset + \".keras\"\n",
        "if os.path.exists( FICHERO ):\n",
        "    # Si el archivo existe, cargar el modelo y continuar con el entrenamiento\n",
        "    modelo            = load_model( FICHERO, compile = True )  # Cargar el modelo con la configuración de compilación\n",
        "    CLASES, nombreClases = obtenerNumeroClases( nombreDataset ), obtenerNombreClases( nombreDataset )\n",
        "\n",
        "    datasetTotal = cargarDataset( nombreDataset, SPLITS )\n",
        "    TOTAL = contarTuplasDataset( nombreDataset, SPLITS )\n",
        "    print( \"Modelo \", FICHERO , \" con imágenes ya ajustadas.\" )\n",
        "\n",
        "    # Preprocesamiento del dataset de entrenamiento y prueba\n",
        "    NUM_ENTRENAMIENTO = int(TOTAL * TRAIN)\n",
        "\n",
        "    datasetTotal = datasetTotal.shuffle(buffer_size=TOTAL, reshuffle_each_iteration=False)\n",
        "    datosEntrenamiento, datosPrueba = datasetTotal.take(NUM_ENTRENAMIENTO), datasetTotal.skip(NUM_ENTRENAMIENTO)\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "    # 3.  ACELERAMOS EL FUNCIONAMIENTO METIENDO LOS DATOS EN CACHÉ\n",
        "    #-------------------------------------------------------------------------------\n",
        "    #datosEntrenamiento = datosEntrenamiento.map(NORMALIZAR).cache()\n",
        "    datosEntrenamiento = datosEntrenamiento.map(lambda img, lbl: NORMALIZAR(img, lbl, TAMAÑO_IMAGEN)).cache()\n",
        "    datosEntrenamiento = datosEntrenamiento.shuffle(buffer_size=NUM_ENTRENAMIENTO).repeat().batch(TAMAÑO_LOTE)\n",
        "\n",
        "    #datosPrueba = datosPrueba.map(NORMALIZAR).cache().batch(TAMAÑO_LOTE)\n",
        "    datosPrueba = datosPrueba.map(lambda img, lbl: NORMALIZAR(img, lbl, TAMAÑO_IMAGEN)).cache().batch(TAMAÑO_LOTE)\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "    # 5.  COMPILACIÓN DEL MODELO\n",
        "    #-------------------------------------------------------------------------------\n",
        "    compilarModelo( modelo, 'adam', 'sparse_categorical_crossentropy', 'accuracy', True )\n",
        "\n",
        "else:\n",
        "    # Si el archivo no existe, continuar con el flujo normal\n",
        "    #nombreDataset, info        = obtenerNombreDataset( SPLITS )\n",
        "    numeroClases, nombreClases = obtenerNumeroClases( nombreDataset ), obtenerNombreClases( nombreDataset )\n",
        "\n",
        "    datasetTotal = cargarDataset( nombreDataset, SPLITS )\n",
        "    TOTAL        = contarTuplasDataset( nombreDataset, SPLITS )\n",
        "\n",
        "    mostrarTexto( f\"|   Total de ejemplos en el dataset cargado: {TOTAL}   |\" )\n",
        "    print(f\"Clases disponibles: {numeroClases}: {nombreClases}\")\n",
        "\n",
        "    # Agrupamos los datos en Entrenamiento (60%) y Prueba (40%)\n",
        "    NUM_ENTRENAMIENTO = int( TOTAL * TRAIN )\n",
        "\n",
        "    # Dividimos el dataset en entrenamiento, validación y prueba\n",
        "    datasetTotal  = datasetTotal.shuffle( buffer_size = TOTAL, reshuffle_each_iteration = False )  # Mezclar el dataset\n",
        "    datosEntrenamiento, datosPrueba = datasetTotal.take( NUM_ENTRENAMIENTO ), datasetTotal.skip( NUM_ENTRENAMIENTO )\n",
        "\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "    # 3.  ACELERAMOS EL FUNCIONAMIENTO METIENDO LOS DATOS EN CACHÉ\n",
        "    #-------------------------------------------------------------------------------\n",
        "    # Preprocesamiento del dataset de entrenamiento\n",
        "    datosEntrenamiento = datosEntrenamiento.map(lambda img, lbl: NORMALIZAR(img, lbl, TAMAÑO_IMAGEN)).cache()\n",
        "\n",
        "    # Configurar los datos para el entrenamiento\n",
        "    datosEntrenamiento = datosEntrenamiento.shuffle(buffer_size=NUM_ENTRENAMIENTO).repeat().batch(TAMAÑO_LOTE)\n",
        "\n",
        "    # Preprocesamiento del dataset de prueba\n",
        "    #datosPrueba = datosPrueba.map(NORMALIZAR).cache().batch(TAMAÑO_LOTE)\n",
        "    datosPrueba = datosPrueba.map(lambda img, lbl: NORMALIZAR(img, lbl, TAMAÑO_IMAGEN)).cache().batch(TAMAÑO_LOTE)\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "    # 4.  CREACIÓN DEL MODELO\n",
        "    #-------------------------------------------------------------------------------\n",
        "    #modelo = crearModelo( numeroClases, INPUT_SHAPE, FILTROS, TAMAÑO_KERNEL, TAMAÑO_POOL, ACTIVACION_TIPO, ACTIVACION_SALIDA, DROPOUT )\n",
        "    #modelo = crearModelo_v1( numeroClases, INPUT_SHAPE, FILTROS, TAMAÑO_KERNEL, TAMAÑO_POOL, ACTIVACION_TIPO, ACTIVACION_SALIDA, DROPOUT )\n",
        "    modelo = crearModelo_v2( numeroClases, INPUT_SHAPE, FILTROS, TAMAÑO_KERNEL, TAMAÑO_POOL, ACTIVACION_TIPO, ACTIVACION_SALIDA, DROPOUT )\n",
        "\n",
        "    #-------------------------------------------------------------------------------\n",
        "    # 5.  COMPILACIÓN DEL MODELO\n",
        "    #-------------------------------------------------------------------------------\n",
        "    compilarModelo( modelo, 'adam', 'sparse_categorical_crossentropy', 'accuracy', True )\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# 6.  ENTRENAMIENTO DEL MODELO\n",
        "#-------------------------------------------------------------------------------\n",
        "FICHERO = nombreDataset + '.keras'\n",
        "STEPS_PER_EPOCH = math.ceil( NUM_ENTRENAMIENTO / TAMAÑO_LOTE ) # lotes que se ejecutarán por cada época\n",
        "\n",
        "mostrarTexto( f\"|  Lado de las imágenes     :  {LADO}  |\" )\n",
        "mostrarTexto( f\"|  Objetivo de entrenamiento:  accuracy = {ACIERTOS}  |\" )\n",
        "\n",
        "entrenarModelo( modelo, ACIERTOS, FICHERO, datosEntrenamiento, 1, STEPS_PER_EPOCH, True )\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# 7. EVALUACIÓN DEL MODELO\n",
        "#-------------------------------------------------------------------------------\n",
        "evaluarModelo( modelo, datosPrueba, True )\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# 8. Hacemos predicciones (modelo, datosPrueba, nombreClases )\n",
        "#-------------------------------------------------------------------------------\n",
        "realizarPedicciones( modelo, datosPrueba, nombreClases, True )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf_master_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
